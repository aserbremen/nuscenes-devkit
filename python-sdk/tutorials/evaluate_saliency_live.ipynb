{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-panoptic...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "34149 panoptic,\n",
      "Done loading in 49.213 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 6.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import subprocess\n",
    "from typing import Tuple, List\n",
    "import os.path as osp\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import points_in_box\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes.utils.geometry_utils import BoxVisibility\n",
    "from nuscenes.panoptic.panoptic_utils import paint_panop_points_label\n",
    "from nuscenes.lidarseg.lidarseg_utils import paint_points_label\n",
    "from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud\n",
    "\n",
    "import numpy as np\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "import cv2\n",
    "\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot='/data/datasets/nuscenes', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(smap):\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    # This operator uses the global\n",
    "    # maximum of each map to obtain a dynamic range between\n",
    "    # 0 and 1\n",
    "   \n",
    "    if smap.min() <0 :\n",
    "        sal_map=smap+np.abs(smap.min())\n",
    "    else:\n",
    "        sal_map=smap\n",
    "        \n",
    "    sal_map=sal_map/sal_map.max()\n",
    "    \n",
    "    return sal_map\n",
    "    \n",
    "def normalized_and_sum(smap1, smap2):\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    \n",
    "    n_smap1 = normalize(smap1)\n",
    "    n_smap2 = normalize(smap2)\n",
    "    \n",
    "    raw_sum_map = n_smap1 + n_smap2\n",
    "    \n",
    "    outmap = normalize(raw_sum_map)\n",
    "    \n",
    "    return outmap\n",
    "\n",
    "\n",
    "def normalized_and_maximum(smap1, smap2):\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    \n",
    "    n_smap1 = normalize(smap1)\n",
    "    n_smap2 = normalize(smap2)\n",
    "    \n",
    "    outmap = np.maximum(n_smap1, n_smap2)\n",
    "\n",
    "    return outmap\n",
    "    \n",
    "def simple_pixel_wise_product(map_a, map_b):\n",
    "    comb_map=map_a*map_b\n",
    "    \n",
    "    #normalize\n",
    "    comb_map=normalize(comb_map)\n",
    "    \n",
    "    return comb_map\n",
    "\n",
    "def coherent_normalization_with_weighted_sum(smap1, smap2, alpha=0.5, beta=1.0):\n",
    "    # REF: https://ieeexplore.ieee.org/document/6605606\n",
    "    \n",
    "    outmap = (1 - alpha)*smap1 + alpha*smap2 + beta*( smap1*smap2)\n",
    "    \n",
    "    return normalize(outmap)\n",
    "\n",
    "def average_local_maxima(smap):\n",
    "    # t = time.time()\n",
    "    local_max_loc=peak_local_max(smap)\n",
    "    # print(\"peak local max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    #all local maxima\n",
    "    all_local_max = smap[local_max_loc[:,0], local_max_loc[:,1]]\n",
    "    # print(\"all local max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    # drop the absolute maximum\n",
    "    other_local_max = np.delete(all_local_max, np.argmax(all_local_max))\n",
    "    # print(\"drop the absolute maximum max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    mean = np.mean(other_local_max)\n",
    "    # print(\"mean in average local maxima {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    return mean\n",
    "\n",
    "def global_nonlinear_amplification(smap1, smap2):\n",
    "    # original REF: http://authors.library.caltech.edu/71459/1/161_1.pdf\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    # REF for local maxima: https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_peak_local_max.html\n",
    "    \n",
    "    # t = time.time()\n",
    "    n_smap1 = normalize(smap1)\n",
    "    # print(\"normalize 1 maximum max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    n_smap2 = normalize(smap2)\n",
    "    # print(\"normalize 2 maximum max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    # follows notation of Le Meur \n",
    "    M_1 = n_smap1.max()\n",
    "    # print(\"max of normalized saliency 1 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    m_1 = average_local_maxima (smap1)\n",
    "    # print(\"average local maxima total 1 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "\n",
    "    M_2 = n_smap2.max()\n",
    "    # print(\"max of normalized saliency 2 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    m_2 = average_local_maxima (smap2)\n",
    "    # print(\"average local maxima total 2 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    \n",
    "    outmap = ( n_smap1 * (M_1-m_1)**2 ) + ( n_smap2 * (M_2-m_2 )**2 )\n",
    "    # print(\"outmap {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    return outmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "font_scale = 0.5\n",
    "thickness = 1\n",
    "\n",
    "\n",
    "def cv2_put_multi_line_text(im: np.ndarray,\n",
    "                            text: str,\n",
    "                            center: Tuple,\n",
    "                            color: Tuple,\n",
    "                            font=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                            font_scale=0.5,\n",
    "                            thickness=1) -> None:\n",
    "    text_size, _ = cv2.getTextSize(\"dummy_text\", font, font_scale, thickness)\n",
    "    line_height = text_size[1] + 5\n",
    "    y0 = int(center[1])\n",
    "    for i, text_line in enumerate(text.split('\\n')):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(im, text_line, (int(center[0]), y),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, font_scale, color[::-1], thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_pointcloud_to_box(nusc: NuScenes,\n",
    "                        sample_token: str,\n",
    "                        pointsensor_token: str,\n",
    "                        camera_token: str,\n",
    "                        box: Box,\n",
    "                        # point_sensor_channel: str ='LIDAR_TOP',\n",
    "                        # camera_channel: str = 'CAM_FRONT',\n",
    "                        min_dist: float = 1.0,\n",
    "                        render_intensity: bool = False,\n",
    "                        show_lidarseg: bool = False,\n",
    "                        filter_lidarseg_labels: List = None,\n",
    "                        lidarseg_preds_bin_path: str = None,\n",
    "                        show_panoptic: bool = False) -> Tuple:\n",
    "    \"\"\"\n",
    "    Given a point sensor (lidar/radar) token and camera sample_data token, load pointcloud and map it to the image\n",
    "    plane.\n",
    "    :param nusc: Active NuScenes object.\n",
    "    :param sample_token: Sample token.\n",
    "    :param pointsensor_token: Lidar/radar sample_data token.\n",
    "    :param camera_token: Camera sample_data token.\n",
    "    :param box: Box that we want map points to.\n",
    "    :param min_dist: Distance from the camera below which points are discarded.\n",
    "    :param render_intensity: Whether to render lidar intensity instead of point depth.\n",
    "    :param show_lidarseg: Whether to render lidar intensity instead of point depth.\n",
    "    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes. If None\n",
    "        or the list is empty, all classes will be displayed.\n",
    "    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation\n",
    "                                    predictions for the sample.\n",
    "    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set\n",
    "        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.\n",
    "        If show_lidarseg is True, show_panoptic will be set to False.\n",
    "    :return (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n",
    "    \"\"\"\n",
    "\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "    cam = nusc.get('sample_data', camera_token)\n",
    "    pcl_path = osp.join(nusc.dataroot, pointsensor['filename'])\n",
    "    if pointsensor['sensor_modality'] == 'lidar':\n",
    "        if show_lidarseg or show_panoptic:\n",
    "            gt_from = 'lidarseg' if show_lidarseg else 'panoptic'\n",
    "            assert hasattr(nusc.nusc, gt_from), f'Error: nuScenes-{gt_from} not installed!'\n",
    "\n",
    "            # Ensure that lidar pointcloud is from a keyframe.\n",
    "            assert pointsensor['is_key_frame'], \\\n",
    "                'Error: Only pointclouds which are keyframes have lidar segmentation labels. Rendering aborted.'\n",
    "\n",
    "            assert not render_intensity, 'Error: Invalid options selected. You can only select either ' \\\n",
    "                                            'render_intensity or show_lidarseg, not both.'\n",
    "\n",
    "        pc = LidarPointCloud.from_file(pcl_path)\n",
    "    else:\n",
    "        pc = RadarPointCloud.from_file(pcl_path)\n",
    "    # im = Image.open(osp.join(nusc.dataroot, cam['filename']))\n",
    "\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = pc.points[2, :]\n",
    "\n",
    "    if render_intensity:\n",
    "        assert pointsensor['sensor_modality'] == 'lidar', 'Error: Can only render intensity for lidar, ' \\\n",
    "                                                            'not %s!' % pointsensor['sensor_modality']\n",
    "        # Retrieve the color from the intensities.\n",
    "        # Performs arbitary scaling to achieve more visually pleasing results.\n",
    "        intensities = pc.points[3, :]\n",
    "        intensities = (intensities - np.min(intensities)) / (np.max(intensities) - np.min(intensities))\n",
    "        intensities = intensities ** 0.1\n",
    "        intensities = np.maximum(0, intensities - 0.5)\n",
    "        coloring = intensities\n",
    "    elif show_lidarseg or show_panoptic:\n",
    "        assert pointsensor['sensor_modality'] == 'lidar', 'Error: Can only render lidarseg labels for lidar, ' \\\n",
    "                                                            'not %s!' % pointsensor['sensor_modality']\n",
    "\n",
    "        gt_from = 'lidarseg' if show_lidarseg else 'panoptic'\n",
    "        semantic_table = getattr(nusc.nusc, gt_from)\n",
    "\n",
    "        if lidarseg_preds_bin_path:\n",
    "            sample_token = nusc.get('sample_data', pointsensor_token)['sample_token']\n",
    "            lidarseg_labels_filename = lidarseg_preds_bin_path\n",
    "            assert os.path.exists(lidarseg_labels_filename), \\\n",
    "                'Error: Unable to find {} to load the predictions for sample token {} (lidar ' \\\n",
    "                'sample data token {}) from.'.format(lidarseg_labels_filename, sample_token, pointsensor_token)\n",
    "        else:\n",
    "            if len(semantic_table) > 0:  # Ensure {lidarseg/panoptic}.json is not empty (e.g. in case of v1.0-test).\n",
    "                lidarseg_labels_filename = osp.join(nusc.dataroot,\n",
    "                                                    nusc.get(gt_from, pointsensor_token)['filename'])\n",
    "            else:\n",
    "                lidarseg_labels_filename = None\n",
    "\n",
    "        if lidarseg_labels_filename:\n",
    "            # Paint each label in the pointcloud with a RGBA value.\n",
    "            if show_lidarseg:\n",
    "                coloring = paint_points_label(lidarseg_labels_filename,\n",
    "                                                filter_lidarseg_labels,\n",
    "                                                nusc.lidarseg_name2idx_mapping,\n",
    "                                                nusc.colormap)\n",
    "            else:\n",
    "                coloring = paint_panop_points_label(lidarseg_labels_filename,\n",
    "                                                    filter_lidarseg_labels,\n",
    "                                                    nusc.lidarseg_name2idx_mapping,\n",
    "                                                    nusc.colormap)\n",
    "\n",
    "        else:\n",
    "            coloring = depths\n",
    "            print(f'Warning: There are no lidarseg labels in {nusc.version}. Points will be colored according '\n",
    "                    f'to distance from the ego vehicle instead.')\n",
    "    else:\n",
    "        # Retrieve the color from the depth.\n",
    "        coloring = depths\n",
    "\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths > min_dist)\n",
    "    # mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    # mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    # mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    # mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    height = cam['height']\n",
    "    width = cam['width']\n",
    "    mask = np.logical_and(mask, points[0, :] > 0)\n",
    "    mask = np.logical_and(mask, points[0, :] < width)\n",
    "    mask = np.logical_and(mask, points[1, :] > 0)\n",
    "    mask = np.logical_and(mask, points[1, :] < height)\n",
    "    # Also only take only points inside the box into account\n",
    "    mask = np.logical_and(mask, points_in_box(box, pc.points[:3, :]))\n",
    "    \n",
    "    points = points[:, mask]\n",
    "    coloring = coloring[mask]\n",
    "\n",
    "    return points, coloring #, im\n",
    "\n",
    "\n",
    "def render_pointcloud_to_box(im: np.ndarray, \n",
    "                             nusc: NuScenes,\n",
    "                             sample_token: str,\n",
    "                             pointsensor_token: str,\n",
    "                             camera_token: str,\n",
    "                             box: Box,\n",
    "                             # point_sensor_channel: str ='LIDAR_TOP',\n",
    "                             # camera_channel: str = 'CAM_FRONT',\n",
    "                             min_dist: float = 1.0,\n",
    "                             render_intensity: bool = False,\n",
    "                             show_lidarseg: bool = False,\n",
    "                             filter_lidarseg_labels: List = None,\n",
    "                             lidarseg_preds_bin_path: str = None,\n",
    "                             show_panoptic: bool = False) -> Tuple:\n",
    "    \"\"\"\n",
    "    Given a point sensor (lidar/radar) token and camera sample_data token, load pointcloud and map it to the image\n",
    "    plane.\n",
    "    :param nusc: Active NuScenes object.\n",
    "    :param sample_token: Sample token.\n",
    "    :param pointsensor_token: Lidar/radar sample_data token.\n",
    "    :param camera_token: Camera sample_data token.\n",
    "    :param box: Box that we want map points to.\n",
    "    :param min_dist: Distance from the camera below which points are discarded.\n",
    "    :param render_intensity: Whether to render lidar intensity instead of point depth.\n",
    "    :param show_lidarseg: Whether to render lidar intensity instead of point depth.\n",
    "    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes. If None\n",
    "        or the list is empty, all classes will be displayed.\n",
    "    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation\n",
    "                                    predictions for the sample.\n",
    "    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set\n",
    "        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.\n",
    "        If show_lidarseg is True, show_panoptic will be set to False.\n",
    "    :return (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n",
    "    \"\"\"\n",
    "    points, coloring = map_pointcloud_to_box(\n",
    "        nusc, sample_token=sample_token, pointsensor_token=pointsensor_token, camera_token=camera_token, box=box, min_dist=min_dist,\n",
    "        render_intensity=render_intensity, show_lidarseg=show_lidarseg, filter_lidarseg_labels=filter_lidarseg_labels,\n",
    "        lidarseg_preds_bin_path=lidarseg_preds_bin_path, show_panoptic=show_panoptic)\n",
    "    # Need to be finished later for more precise contours of tracked objects and new objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pixels_from_box_cv2(box: Box,\n",
    "                             width: int,\n",
    "                             height: int,\n",
    "                             view: np.ndarray = np.eye(3),\n",
    "                             normalize: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Renders box using OpenCV2.\n",
    "    :param Box: Box to be rendered\n",
    "    :param width: width of the image\n",
    "    :param height: height of the image\n",
    "    :param view: camera intrinsics\n",
    "    :param normalize: Whether to normalize the remaining coordinate.\n",
    "    \"\"\"\n",
    "\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :]  # all x, y picture coordinates\n",
    "\n",
    "    # simply fill all 6 sides of the cuboid without differentiating which sides are in the fore- or background\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    front = np.ix_([0, 1, 2, 3], [0, 1])\n",
    "    rear = np.ix_([4, 5, 6, 7], [0, 1])\n",
    "    bottom = np.ix_([2, 3, 7, 6], [0, 1])\n",
    "    top = np.ix_([0, 1, 5, 4], [0, 1])\n",
    "    left = np.ix_([0, 3, 7, 4], [0, 1])\n",
    "    right = np.ix_([1, 2, 6, 5], [0, 1])\n",
    "    cv2.fillPoly(mask, pts=[corners.T[front].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[rear].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[bottom].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[top].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[left].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[right].astype(int)], color=(255))\n",
    "\n",
    "    pixels_tuple = np.where(mask == 255)\n",
    "    return mask, pixels_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbvs_bin = \"/home/serov/code/cpp/GBVS_fork/build/gbvs_main\"\n",
    "\n",
    "def get_gbvs_cpp(input_filename):\n",
    "    output_filename = input_filename.replace('image.png', 'gbvs.exr')\n",
    "    gbvs_cmd = gbvs_bin + \" -i \" + input_filename + \" -o \" + output_filename\n",
    "    p = subprocess.Popen(gbvs_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if p.wait() == 0:\n",
    "        return cv2.imread(output_filename, flags=(cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH) )\n",
    "    else:\n",
    "        raise Exception(\"Couldn't create GBVS saliency map.\")\n",
    "\n",
    "def create_gbvs_cpp(input_filename):\n",
    "    output_filename = input_filename.replace('image.png', 'gbvs.exr')\n",
    "    gbvs_cmd = gbvs_bin + \" -i \" + input_filename + \" -o \" + output_filename\n",
    "    p = subprocess.Popen(gbvs_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if p.wait() == 0:\n",
    "        print(\"Created gbvs map at:\", output_filename)\n",
    "    else:\n",
    "        raise Exception(\"Couldn't create GBVS saliency map.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_absolute_error(saliency: np.ndarray,\n",
    "                        groundtruth: np.ndarray) -> float:\n",
    "    assert(saliency.shape == groundtruth.shape)\n",
    "    height, width = saliency.shape\n",
    "    dtype_saliency = saliency.dtype\n",
    "    return 1 / (height * width) * np.sum(np.absolute(saliency - np.asarray(groundtruth / 255.0, dtype=dtype_saliency)))\n",
    "\n",
    "def mean_absolute_error_new(saliency: np.ndarray,\n",
    "                            mask: np.ndarray):\n",
    "    assert(saliency.shape == mask.shape)\n",
    "    height, width = saliency.shape\n",
    "    dtype_saliency = saliency.dtype\n",
    "    \n",
    "    indices_new_obj = np.where(mask == 255)\n",
    "    num_pixels_new = len(indices_new_obj[0])\n",
    "    saliency_new = saliency[[indices_new_obj[0], indices_new_obj[1]]]\n",
    "    mae_new_only = 1 / num_pixels_new * np.sum(np.absolute(saliency_new - 1))\n",
    "\n",
    "    mae_new_im = 1 / (height * width) * np.sum(np.absolute(saliency - np.asarray(mask.clip(0, 1).astype(dtype_saliency)))) \n",
    "    \n",
    "    return mae_new_only, mae_new_im\n",
    "\n",
    "def mean_absolute_error_multi(saliency: np.ndarray,\n",
    "                              mask_new: np.ndarray,\n",
    "                              mask_already: np.ndarray) -> tuple:\n",
    "    assert(saliency.shape == mask_new.shape)\n",
    "    assert(mask_new.shape == mask_already.shape)\n",
    "    height, width = saliency.shape\n",
    "    dtype_saliency = saliency.dtype\n",
    "    indices_new_obj = np.where(mask_new == 255)\n",
    "    num_pixels_new = len(indices_new_obj[0])\n",
    "    saliency_new = saliency[[indices_new_obj[0], indices_new_obj[1]]]\n",
    "\n",
    "    mae_new = 1 / num_pixels_new * np.sum(np.absolute(saliency_new - 1))\n",
    "    mae_already = 1 / (height * width) * np.sum(np.absolute(saliency - np.asarray(mask_already.clip(0, 1).astype(dtype_saliency))))\n",
    "    \n",
    "    return mae_new, mae_already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_image_and_saliency_as_heatmap(im: np.ndarray,\n",
    "                                          saliency: np.ndarray,\n",
    "                                          resize_factor: float) -> np.ndarray:\n",
    "\n",
    "    im_resize = cv2.resize(im, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    saliency_resize = cv2.resize(saliency, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    if im_resize.dtype != np.uint8:\n",
    "        im_resize = np.asarray(im_resize, dtype=np.uint8)\n",
    "    if saliency_resize.dtype != np.uint8:\n",
    "        saliency_resize = np.asarray(saliency_resize * 255, dtype=np.uint8)\n",
    "    if len(im_resize.shape) != len(saliency_resize.shape):\n",
    "        if len(saliency_resize.shape) == 2:  # grayscale\n",
    "            saliency_resize = cv2.cvtColor(saliency_resize, cv2.COLOR_GRAY2BGR)  # convert to color\n",
    "        elif len(saliency_resize).shape == 3:  # color\n",
    "            saliency_resize = cv2.cvtColor(saliency_resize, cv2.COLOR_BGR2GRAY)  # convert to grayscale\n",
    "\n",
    "    saliency_resize = cv2.applyColorMap(saliency_resize, cv2.COLORMAP_JET)\n",
    "\n",
    "    alpha = 0.7\n",
    "    beta = 0.3\n",
    "    gamma = 0\n",
    "    return cv2.addWeighted(im_resize, alpha, saliency_resize, beta, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convex_mask_and_hull_points_from_lidar_points(box_mask: np.ndarray,\n",
    "                           lidar_points_inside_box: np.ndarray,\n",
    "                           width: int,\n",
    "                           height: int) -> Tuple:\n",
    "    # check number of lidar points and overall size of the masked_box vs. the contour created from projected lidar points\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    hull_points = cv2.convexHull(lidar_points_inside_box[:2, :].T.astype(np.float32))\n",
    "  \n",
    "    if hull_points is None:\n",
    "        hull_valid = False\n",
    "    else:\n",
    "        hull_valid = True\n",
    "        # if scale != 1.0:\n",
    "        #     M = cv2.moments(hull_points)\n",
    "        #     cx = int(M['m10']/M['m00'])\n",
    "        #     cy = int(M['m01']/M['m00'])\n",
    "        #     hull_points_norm = hull_points - [cx, cy]\n",
    "        #     hull_points_scaled = hull_points_norm * scale\n",
    "        #     hull_points = hull_points_scaled + [cx, cy]\n",
    "        cv2.fillConvexPoly(mask, hull_points.astype(np.int32), color=(255))\n",
    "\n",
    "    return mask, hull_valid, hull_points\n",
    "\n",
    "def scale_contour(contour: np.ndarray,\n",
    "                  scale: float)-> np.ndarray:\n",
    "    if scale == 1.0:\n",
    "        return contour\n",
    "    else:\n",
    "        M = cv2.moments(contour)\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "        contour_norm = contour - [[cx], [cy]]\n",
    "        contour_scaled = scale * contour_norm \n",
    "        # contour_scaled += [[cy], [cx]]\n",
    "        return contour_scaled, (cy, cy)\n",
    "\n",
    "\n",
    "def center_of_hull_points(hull_points: np.ndarray):\n",
    "    M = cv2.moments(hull_points)\n",
    "    if M['m00'] == 0:\n",
    "        return (None, None)\n",
    "    else:\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "        return (cx, cy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "def box_render_cv2_text(box: Box,\n",
    "                        nusc: NuScenes,\n",
    "                        im: np.ndarray,\n",
    "                        view: np.ndarray = np.eye(3),\n",
    "                        normalize: bool = False,\n",
    "                        colors: Tuple = ((0, 0, 255), (255, 0, 0), (155, 155, 155)),\n",
    "                        linewidth: int = 2,\n",
    "                        render_text: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Renders box using OpenCV2.\n",
    "    :param Box: Box to be rendered\n",
    "    :param nusc: Active NuScenes object\n",
    "    :param im: <np.array: width, height, 3>. Image array. Channels are in BGR order.\n",
    "    :param view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).\n",
    "    :param is_key_frame\n",
    "    :param normalize: Whether to normalize the remaining coordinate.\n",
    "    :param colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.\n",
    "    :param linewidth: Linewidth for plot.\n",
    "    \"\"\"\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :]\n",
    "\n",
    "    def draw_rect(selected_corners, color):\n",
    "        prev = selected_corners[-1]\n",
    "        for corner in selected_corners:\n",
    "            cv2.line(im,\n",
    "                     (int(prev[0]), int(prev[1])),\n",
    "                     (int(corner[0]), int(corner[1])),\n",
    "                     color, linewidth)\n",
    "            prev = corner\n",
    "\n",
    "    # Draw the sides\n",
    "    for i in range(4):\n",
    "        cv2.line(im,\n",
    "                 (int(corners.T[i][0]), int(corners.T[i][1])),\n",
    "                 (int(corners.T[i + 4][0]), int(corners.T[i + 4][1])),\n",
    "                 colors[2][::-1], linewidth)\n",
    "\n",
    "    # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n",
    "    draw_rect(corners.T[:4], colors[0][::-1])\n",
    "    draw_rect(corners.T[4:], colors[1][::-1])\n",
    "\n",
    "    # Draw line indicating the front\n",
    "    center_bottom_forward = np.mean(corners.T[2:4], axis=0)\n",
    "    center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)\n",
    "    cv2.line(im,\n",
    "             (int(center_bottom[0]), int(center_bottom[1])),\n",
    "             (int(center_bottom_forward[0]), int(center_bottom_forward[1])),\n",
    "             colors[0][::-1], linewidth)\n",
    "\n",
    "    # Add some text to the bounding box\n",
    "    # Create text to be written\n",
    "    attribute_tokens =  nusc.get('sample_annotation', box.token)['attribute_tokens']\n",
    "    text = box.name\n",
    "    for attribute_token in attribute_tokens:\n",
    "        attribute = nusc.get('attribute', attribute_token)\n",
    "        text += '\\n' + attribute['name']\n",
    "    visibility = nusc.get('sample_annotation', box.token)['visibility_token']\n",
    "    text += '\\nvisibility ' + visibility\n",
    "    # Setup multi line text\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    font_scale = 0.5\n",
    "    thickness = 1\n",
    "    text_size, _ = cv2.getTextSize(box.name, font, font_scale, thickness)\n",
    "    line_height = text_size[1] + 5\n",
    "    center = np.mean(corners.T[:], axis=0)\n",
    "    y0 = int(center[1])\n",
    "    for i, text_line in enumerate(text.split('\\n')):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(im, text_line, (int(center[0]), y),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, font_scale, colors[0][::-1], thickness, cv2.LINE_AA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningStats():\n",
    "    \"\"\" \n",
    "    see https://github.com/liyanage/python-modules/blob/master/running_stats.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.old_m = 0\n",
    "        self.new_m = 0\n",
    "        self.old_s = 0\n",
    "        self.new_s = 0\n",
    "        self.categories = []\n",
    "        self.mean_absolute_error_new_objects = []\n",
    "        self.mean_absolute_error_already_tracked_objects = []\n",
    "        self.mean_absolute_error_whole_image = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def add_sample(self, x):\n",
    "        self.n += 1\n",
    "\n",
    "        if self.n == 1:\n",
    "            self.old_m = self.new_m = x\n",
    "            self.old_s = 0\n",
    "        else:\n",
    "            self.new_m = self.old_m + (x - self.old_m) / self.n\n",
    "            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n",
    "\n",
    "            self.old_m = self.new_m\n",
    "            self.old_s = self.new_s\n",
    "\n",
    "    def mean(self):\n",
    "        return self.new_m if self.n else 0.0\n",
    "\n",
    "    def variance(self):\n",
    "        return self.new_s / (self.n - 1) if self.n > 1 else 0.0\n",
    "\n",
    "    def standard_deviation(self):\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "class SaliencyEvalStats():\n",
    "    def __init__(self, name) -> None:\n",
    "        # arrays containing relevant data\n",
    "        self.n = int(0)\n",
    "        self.name = name\n",
    "        self.mean_absolute_error_new_objects: float = []\n",
    "        self.mean_absolute_error_already_tracked_objects: float = []\n",
    "    \n",
    "    def add_sample(self, mean_absolute_error_new_object, mean_absolute_error_already_tracked_object):\n",
    "        self.n += 1\n",
    "        self.mean_absolute_error_new_objects.append(mean_absolute_error_new_object)\n",
    "        self.mean_absolute_error_already_tracked_objects.append(mean_absolute_error_already_tracked_object)\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(\"{:30} n: {:d}  mae_new {:0.10f}  std_new  {:0.10f}  mae_tracked  {:0.10f}  std_tracked  {:0.10f}\".format(\n",
    "            self.name, self.n, np.mean(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "            np.std(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "            np.mean(np.asarray(self.mean_absolute_error_already_tracked_objects)),\n",
    "            np.std(np.asarray(self.mean_absolute_error_already_tracked_objects))))\n",
    "\n",
    "    def get_stats(self):\n",
    "        return np.array([[np.mean(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "                          np.std(np.asarray(self.mean_absolute_error_new_objects))]\n",
    "                         [np.mean(np.asarray(self.mean_absolute_error_already_tracked_objects)),\n",
    "                          np.std(np.asarray(self.mean_absolute_error_already_tracked_objects))]])\n",
    "\n",
    "    def n(self):\n",
    "        return self.n\n",
    "\n",
    "\n",
    "class SaliencyEvalNewStats():\n",
    "    def __init__(self, name) -> None:\n",
    "        # arrays containing relevant data\n",
    "        self.n = int(0)\n",
    "        self.name = name\n",
    "        self.mean_absolute_error_new_objects: float = []\n",
    "        self.mean_absolute_error_new_objects_whole_im: float = []\n",
    "\n",
    "    def add_sample(self, mean_absolute_error_new_object, mean_absolute_error_new_object_whole_im):\n",
    "        self.n += 1\n",
    "        self.mean_absolute_error_new_objects.append(mean_absolute_error_new_object)\n",
    "        self.mean_absolute_error_new_objects_whole_im.append(mean_absolute_error_new_object_whole_im)\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(\"{:30} n: {:d}  mae_new {:0.10f}  std_new  {:0.10f}  mae_new_im  {:0.10f}  std_new_im  {:0.10f}\".format(\n",
    "            self.name, self.n, np.mean(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "            np.std(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "            np.mean(np.asarray(self.mean_absolute_error_new_objects_whole_im)),\n",
    "            np.std(np.asarray(self.mean_absolute_error_new_objects_whole_im))))\n",
    "\n",
    "    def get_stats(self):\n",
    "        return np.array([[np.mean(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "                          np.std(np.asarray(self.mean_absolute_error_new_objects))]\n",
    "                         [np.mean(np.asarray(self.mean_absolute_error_new_objects_whole_im)),\n",
    "                          np.std(np.asarray(self.mean_absolute_error_new_objects_whole_im))]])\n",
    "\n",
    "    def n(self):\n",
    "        return self.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_im_and_saliency(im: np.ndarray,\n",
    "                            saliency: np.ndarray,\n",
    "                            window_name: str,\n",
    "                            # mask_new: np.ndarray,\n",
    "                            # mask_already: np.ndarray,\n",
    "                            # mae_new: float,\n",
    "                            # mae_already: float\n",
    "                            ) -> None:\n",
    "    (height, width, channels) = im.shape\n",
    "    saliency_heatmap = cv2.cvtColor((255*saliency).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    assert(im.shape == saliency_heatmap.shape)\n",
    "    saliency_heatmap = cv2.applyColorMap(saliency_heatmap, cv2.COLORMAP_JET)\n",
    "    display_im = cv2.addWeighted(im, 0.7, saliency_heatmap, 0.3, 0.0)\n",
    "    cv2.imshow(window_name, display_im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyWindow(window_name)\n",
    "\n",
    "def debug_im_and_saliency(im: np.ndarray,\n",
    "                          saliency: np.ndarray,\n",
    "                          window_name: str,\n",
    "                          mae_new: float,\n",
    "                          mae_already: float\n",
    "                          ) -> None:\n",
    "    (height, width, channels) = im.shape\n",
    "    saliency_heatmap = cv2.cvtColor((255*saliency).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    assert(im.shape == saliency_heatmap.shape)\n",
    "    saliency_heatmap = cv2.applyColorMap(saliency_heatmap, cv2.COLORMAP_JET)\n",
    "    display_im = cv2.addWeighted(im, 0.7, saliency_heatmap, 0.3, 0.0)\n",
    "    cv2_put_multi_line_text(display_im, \"mae_new_only = {:0.10f}\".format(mae_new), (width/2, height/2), BLACK)\n",
    "    cv2_put_multi_line_text(display_im, \"mae_new_im = {:0.10f}\".format(mae_already), (width/2, height/2 + 20), BLACK)\n",
    "    cv2.imshow(window_name, display_im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyWindow(window_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating scene  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serov/anaconda3/envs/nuscenes_local/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "camera_channel = 'CAM_FRONT'\n",
    "pointsensor_channel = 'LIDAR_TOP'\n",
    "minimum_visibility = 2 # excluding objects that are only visibile 0-40%\n",
    "height = 900\n",
    "width = 1600\n",
    "break_all = False\n",
    "save_already_tracked = False\n",
    "debug = True\n",
    "debug_single = True\n",
    "\n",
    "data_path = \"/data/datasets/nuscenes_results/birth_frames_and_masks\"\n",
    "\n",
    "# create spectral residual object\n",
    "spectral_residual_h64w64 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h64w64.setImageHeight(64)\n",
    "spectral_residual_h64w64.setImageWidth(64)\n",
    "spectral_residual_h128w128 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h128w128.setImageHeight(128)\n",
    "spectral_residual_h128w128.setImageWidth(128)\n",
    "# h8w64\n",
    "spectral_residual_h8w64 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h8w64.setImageHeight(8)\n",
    "spectral_residual_h8w64.setImageWidth(128)\n",
    "# h8w128\n",
    "spectral_residual_h8w128 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h8w128.setImageHeight(8)\n",
    "spectral_residual_h8w128.setImageWidth(128)\n",
    "# h8w256\n",
    "spectral_residual_h8w256 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h8w256.setImageHeight(8)\n",
    "spectral_residual_h8w256.setImageWidth(256)\n",
    "\n",
    "# prepare baseline birth histogram / saliency\n",
    "baseline_histogram_filename = \"/data/datasets/nuscenes_results/object_birth_heatmaps/birth_heatmap_0849.npy\"\n",
    "baseline_birth_histogram = np.load(baseline_histogram_filename)\n",
    "# make baseline horizontally symmetric\n",
    "baseline_birth_histogram = (baseline_birth_histogram + np.fliplr(baseline_birth_histogram)) / 2\n",
    "baseline_birth_histogram = baseline_birth_histogram / np.max(baseline_birth_histogram)\n",
    "base_saliency = baseline_birth_histogram.copy()\n",
    "baseline_birth_histogram_uint8 = (255 * baseline_birth_histogram).astype(np.uint8)\n",
    "baseline_birth_histogram_heatmap = cv2.cvtColor(baseline_birth_histogram_uint8, cv2.COLOR_GRAY2BGR)\n",
    "baseline_birth_histogram_heatmap = cv2.applyColorMap(baseline_birth_histogram_uint8, cv2.COLORMAP_JET)\n",
    "\n",
    "# solo stats\n",
    "base_stats = SaliencyEvalNewStats(\"base\")\n",
    "gbvs_stats = SaliencyEvalNewStats(\"gbvs\")\n",
    "spectralh64w64_stats = SaliencyEvalNewStats(\"spectralh64w64\")\n",
    "spectralh128w128_stats = SaliencyEvalNewStats(\"spectralh128w128\")\n",
    "spectralh8w64_stats = SaliencyEvalNewStats(\"spectralh8w64\")\n",
    "spectralh8w128_stats = SaliencyEvalNewStats(\"spectralh8w128\")\n",
    "spectralh8w256_stats = SaliencyEvalNewStats(\"spectralh8w256\")\n",
    "# gbvs combinations\n",
    "gbvs_baseline_pwp_stats = SaliencyEvalNewStats(\"gbvs_baseline_pwp\")\n",
    "gbvs_baseline_ns_stats = SaliencyEvalNewStats(\"gbvs_baseline_ns\")\n",
    "gbvs_baseline_nm_stats = SaliencyEvalNewStats(\"gbvs_baseline_nm\")\n",
    "gbvs_baseline_cnws_stats = SaliencyEvalNewStats(\"gbvs_baseline_cnws\")\n",
    "# h64w64 combinations\n",
    "spectralh64w64_baseline_pwp_stats = SaliencyEvalNewStats(\"spectralh64w64_baseline_pwp\")\n",
    "spectralh64w64_baseline_ns_stats = SaliencyEvalNewStats(\"spectralh64w64_baseline_ns\")\n",
    "spectralh64w64_baseline_nm_stats = SaliencyEvalNewStats(\"spectralh64w64_baseline_nm\")\n",
    "spectralh64w64_baseline_cnws_stats = SaliencyEvalNewStats(\"spectralh64w64_baseline_cnws\")\n",
    "# h128w128 combinations\n",
    "spectralh128w128_baseline_pwp_stats = SaliencyEvalNewStats(\"spectralh128w128_baseline_pwp_stats\")\n",
    "spectralh128w128_baseline_ns_stats = SaliencyEvalNewStats(\"spectralh128w128_baseline_ns_stats\")\n",
    "spectralh128w128_baseline_nm_stats = SaliencyEvalNewStats(\"spectralh128w128_baseline_nm_stats\")\n",
    "spectralh128w128_baseline_cnws_stats = SaliencyEvalNewStats(\"spectralh128w128_baseline_cnws_stats\")\n",
    "# h8w64 combinations\n",
    "spectralh8w64_baseline_pwp_stats = SaliencyEvalNewStats(\"spectralh8w64_baseline_pwp_stats\")\n",
    "spectralh8w64_baseline_ns_stats = SaliencyEvalNewStats(\"spectralh8w64_baseline_ns_stats\")\n",
    "spectralh8w64_baseline_nm_stats = SaliencyEvalNewStats(\"spectralh8w64_baseline_nm_stats\")\n",
    "spectralh8w64_baseline_cnws_stats = SaliencyEvalNewStats(\"spectralh8w64_baseline_cnws_stats\")\n",
    "# h8w128 combinations\n",
    "spectralh8w128_baseline_pwp_stats = SaliencyEvalNewStats(\"spectralh8w128_baseline_pwp_stats\")\n",
    "spectralh8w128_baseline_ns_stats = SaliencyEvalNewStats(\"spectralh8w128_baseline_ns_stats\")\n",
    "spectralh8w128_baseline_nm_stats = SaliencyEvalNewStats(\"spectralh8w128_baseline_nm_stats\")\n",
    "spectralh8w128_baseline_cnws_stats = SaliencyEvalNewStats(\"spectralh8w128_baseline_cnws_stats\")\n",
    "# h8w256 combinations\n",
    "spectralh8w256_baseline_pwp_stats = SaliencyEvalNewStats(\"spectralh8w256_baseline_pwp_stats\")\n",
    "spectralh8w256_baseline_ns_stats = SaliencyEvalNewStats(\"spectralh8w256_baseline_ns_stats\")\n",
    "spectralh8w256_baseline_nm_stats = SaliencyEvalNewStats(\"spectralh8w256_baseline_nm_stats\")\n",
    "spectralh8w256_baseline_cnws_stats = SaliencyEvalNewStats(\"spectralh8w256_baseline_cnws_stats\")\n",
    "\n",
    "timestamps: str = []\n",
    "scene_idxs: int = []\n",
    "image_filenames: str = []\n",
    "categories: int = []\n",
    "\n",
    "for idx_scene, scene in enumerate(nusc.scene):\n",
    "    first_sample_rec = nusc.get('sample', scene['first_sample_token'])\n",
    "    first_sd_rec = nusc.get('sample_data', first_sample_rec['data'][camera_channel])\n",
    "    current_sd_rec = first_sd_rec\n",
    "    print(\"evaluating scene \", idx_scene + 1)\n",
    "    has_more_frames = True\n",
    "    unique_instances = [] # list of all unique objects per scene\n",
    "    while has_more_frames:\n",
    "        # Get annotations, boxes, camera_intrinsic\n",
    "        # When using BoxVisibility.ALL objects are detected too late in certain cases\n",
    "        impath, boxes, camera_intrinsic = nusc.get_sample_data(current_sd_rec['token'], box_vis_level=BoxVisibility.ANY)\n",
    "        if not os.path.exists(impath):\n",
    "            raise Exception('Error: Missing image %s' % impath)\n",
    "\n",
    "        current_im = cv2.imread(impath)\n",
    "        current_im_box = current_im.copy()\n",
    "        timestamp = current_sd_rec['timestamp']\n",
    "\n",
    "        already_tracked_objects_mask = np.zeros((height, width), dtype=int)\n",
    "        new_objects_mask = np.zeros((height, width), dtype=int)\n",
    "        im_contains_unique_instance_and_is_not_first_frame = False\n",
    "        new_object_categories_in_keyframe = []\n",
    "        if current_sd_rec['is_key_frame']:\n",
    "            for box in boxes:\n",
    "                sample_annotation = nusc.get('sample_annotation', box.token)\n",
    "                instance = nusc.get('instance', sample_annotation['instance_token'])\n",
    "                category = nusc.get('category', instance['category_token'])\n",
    "                # Exclude certain static objects, e.g. debris or traffic cones\n",
    "                if category['index'] > 8 and category['index'] < 14:\n",
    "                    continue\n",
    "                # Only keep objects with certain visibility\n",
    "                if int(sample_annotation['visibility_token']) < minimum_visibility:\n",
    "                    continue\n",
    "                skip_box_due_to_attribute = False\n",
    "                for attribute_token in sample_annotation['attribute_tokens']:\n",
    "                    attribute_name = nusc.get('attribute', attribute_token)['name']\n",
    "                    if 'without_rider' in attribute_name or 'sitting' in attribute_name:\n",
    "                        skip_box_due_to_attribute = True\n",
    "                if skip_box_due_to_attribute:\n",
    "                    continue\n",
    "                new_object_categories_in_keyframe.append(category)\n",
    "                # Gather unique instances\n",
    "                instance_token = sample_annotation['instance_token']\n",
    "                if instance_token not in unique_instances:\n",
    "                    im_contains_unique_instance = True\n",
    "                    unique_instances.append(instance_token)\n",
    "                    if timestamp != first_sd_rec['timestamp']:\n",
    "                        # This box can mean a potential object birth\n",
    "                        im_contains_unique_instance_and_is_not_first_frame = True\n",
    "                        # full bounding box\n",
    "                        box_mask, _ = mask_pixels_from_box_cv2(box, width, height, view=camera_intrinsic, normalize=True)\n",
    "                        \n",
    "                        new_objects_mask = new_objects_mask + box_mask\n",
    "                        box_render_cv2_text(box, nusc, current_im_box, view=camera_intrinsic, normalize=True, colors=(GREEN, GREEN, GREEN), render_text=False)\n",
    "                        # this will be done in another evaluation\n",
    "                        # new_objects_categories_and_masks.append(tuple([category, box_mask]))\n",
    "                else:\n",
    "                    box_mask, _ = mask_pixels_from_box_cv2(box, width, height, view=camera_intrinsic, normalize=True)\n",
    "                    already_tracked_objects_mask = already_tracked_objects_mask + box_mask\n",
    "                    # c = nusc.explorer.get_color(box.name)\n",
    "                    box_render_cv2_text(box, nusc, current_im_box, view=camera_intrinsic, normalize=True,\n",
    "                                        colors=(RED, RED, RED), render_text=False)\n",
    "\n",
    "            # only evaluate frames that are keyframes, are not first frames and contain new unique instances \n",
    "            if im_contains_unique_instance_and_is_not_first_frame:\n",
    "                timestamps.append(timestamp)\n",
    "                scene_idxs.append(idx_scene + 1)\n",
    "                image_filenames.append(impath)\n",
    "                if len(new_object_categories_in_keyframe) > 1:\n",
    "                    categories.append(42)\n",
    "                else:\n",
    "                    categories.append(category)\n",
    "                # create mask usable in cv2\n",
    "                new_objects_mask = new_objects_mask.clip(0, 255).astype(np.uint8)\n",
    "                already_tracked_objects_mask = already_tracked_objects_mask.clip(0, 255).astype(np.uint8)\n",
    "                if save_already_tracked:\n",
    "                    already_tracked_filename = data_path + os.sep + \"s{:04d}_{}_already_tracked.png\".format(idx_scene + 1, timestamp)\n",
    "                    cv2.imwrite(already_tracked_filename, already_tracked_objects_mask)\n",
    "                # prepare different saliency base maps\n",
    "                gbvs_filename = data_path + os.sep + \"s{:04d}_{}_gbvs.exr\".format(idx_scene + 1, timestamp)\n",
    "                gbvs_saliency = cv2.imread(gbvs_filename, flags=(cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH))\n",
    "                if gbvs_saliency is None:\n",
    "                    print(\"Couldn't read gbvs saliency for \", gbvs_filename)\n",
    "                (spectralh64w64_success, spectralh64w64_saliency) = spectral_residual_h64w64.computeSaliency(current_im)\n",
    "                if not spectralh64w64_success:\n",
    "                    print(\"Couldn't create saliency spectralh64w64 for \", impath)\n",
    "                    continue\n",
    "                (spectralh128w128_success, spectralh128w128_saliency) = spectral_residual_h128w128.computeSaliency(current_im)\n",
    "                if not spectralh128w128_success:\n",
    "                    print(\"Couldn't create saliency spectralh128w128 for \", impath)\n",
    "                    continue\n",
    "                (spectralh8w64_success, spectralh8w64_saliency) = spectral_residual_h8w64.computeSaliency(current_im)\n",
    "                if not spectralh8w64_success:\n",
    "                    print(\"Couldn't create saliency spectralh8w64 for \", impath)\n",
    "                    continue\n",
    "                (spectralh8w128_success, spectralh8w128_saliency) = spectral_residual_h8w128.computeSaliency(current_im)\n",
    "                if not spectralh8w128_success:\n",
    "                    print(\"Couldn't create saliency spectralh8w128 for \", impath)\n",
    "                    continue\n",
    "                (spectralh8w256_success, spectralh8w256_saliency) = spectral_residual_h8w256.computeSaliency(current_im)\n",
    "                if not spectralh8w256_success:\n",
    "                    print(\"Couldn't create saliency spectralh8w1256 for \", impath)\n",
    "                    continue\n",
    "                \n",
    "                # create all gbvs combinations\n",
    "                gbvs_base_pwp_saliency = simple_pixel_wise_product(gbvs_saliency, base_saliency)\n",
    "                gbvs_base_ns_saliency = normalized_and_sum(gbvs_saliency, base_saliency)\n",
    "                gbvs_base_nm_saliency = normalized_and_maximum(gbvs_saliency, base_saliency)\n",
    "                gbvs_base_cnws_saliency = coherent_normalization_with_weighted_sum(gbvs_saliency, base_saliency)\n",
    "                # create spectral h64w64 combinations\n",
    "                spectralh64w64_base_pwp_saliency = simple_pixel_wise_product(spectralh64w64_saliency, base_saliency)\n",
    "                spectralh64w64_base_ns_saliency = normalized_and_sum(spectralh64w64_saliency, base_saliency)\n",
    "                spectralh64w64_base_nm_saliency = normalized_and_maximum(spectralh64w64_saliency, base_saliency)\n",
    "                spectralh64w64_base_cnws_saliency = coherent_normalization_with_weighted_sum(spectralh64w64_saliency, base_saliency)\n",
    "                # create spectral h128w128  combinations\n",
    "                spectralh128w128_base_pwp_saliency = simple_pixel_wise_product(spectralh128w128_saliency, base_saliency)\n",
    "                spectralh128w128_base_ns_saliency = normalized_and_sum(spectralh128w128_saliency, base_saliency)\n",
    "                spectralh128w128_base_nm_saliency = normalized_and_maximum(spectralh128w128_saliency, base_saliency)\n",
    "                spectralh128w128_base_cnws_saliency = coherent_normalization_with_weighted_sum(spectralh128w128_saliency, base_saliency)\n",
    "                # create spectral h8w64  combinations\n",
    "                spectralh8w64_base_pwp_saliency = simple_pixel_wise_product(spectralh8w64_saliency, base_saliency)\n",
    "                spectralh8w64_base_ns_saliency = normalized_and_sum(spectralh8w64_saliency, base_saliency)\n",
    "                spectralh8w64_base_nm_saliency = normalized_and_maximum(spectralh8w64_saliency, base_saliency)\n",
    "                spectralh8w64_base_cnws_saliency = coherent_normalization_with_weighted_sum(spectralh8w64_saliency, base_saliency)\n",
    "                # create spectral h8w128  combinations\n",
    "                spectralh8w128_base_pwp_saliency = simple_pixel_wise_product(spectralh8w128_saliency, base_saliency)\n",
    "                spectralh8w128_base_ns_saliency = normalized_and_sum(spectralh8w128_saliency, base_saliency)\n",
    "                spectralh8w128_base_nm_saliency = normalized_and_maximum(spectralh8w128_saliency, base_saliency)\n",
    "                spectralh8w128_base_cnws_saliency = coherent_normalization_with_weighted_sum(spectralh8w128_saliency, base_saliency)\n",
    "                # create spectral h8w256  combinations\n",
    "                spectralh8w256_base_pwp_saliency = simple_pixel_wise_product(spectralh8w256_saliency, base_saliency)\n",
    "                spectralh8w256_base_ns_saliency = normalized_and_sum(spectralh8w256_saliency, base_saliency)\n",
    "                spectralh8w256_base_nm_saliency = normalized_and_maximum(spectralh8w256_saliency, base_saliency)\n",
    "                spectralh8w256_base_cnws_saliency = coherent_normalization_with_weighted_sum(spectralh8w256_saliency, base_saliency)\n",
    "                \n",
    "                # Calculate solo all errors\n",
    "                mae_new_base, mae_im_base = mean_absolute_error_new(base_saliency, new_objects_mask)\n",
    "                mae_new_gbvs, mae_im_gbvs = mean_absolute_error_new(gbvs_saliency, new_objects_mask)\n",
    "                mae_new_spectralh64w64, mae_im_spectralh64w64 = mean_absolute_error_new(spectralh64w64_saliency, new_objects_mask)\n",
    "                mae_new_spectralh128w128, mae_im_spectralh128w128 = mean_absolute_error_new(spectralh128w128_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w64, mae_im_spectralh8w64 = mean_absolute_error_new(spectralh8w64_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w128, mae_im_spectralh8w128 = mean_absolute_error_new(spectralh8w128_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w256, mae_im_spectralh8w256 = mean_absolute_error_new(spectralh8w256_saliency, new_objects_mask)\n",
    "                # calculate all gbvs combination errors\n",
    "                mae_new_gbvs_base_pwp, mae_im_gbvs_base_pwp = mean_absolute_error_new(gbvs_base_pwp_saliency, new_objects_mask)\n",
    "                mae_new_gbvs_base_ns, mae_im_gbvs_base_ns = mean_absolute_error_new(gbvs_base_ns_saliency, new_objects_mask)\n",
    "                mae_new_gbvs_base_nm, mae_im_gbvs_base_nm = mean_absolute_error_new(gbvs_base_nm_saliency, new_objects_mask)\n",
    "                mae_new_gbvs_base_cnws, mae_im_gbvs_base_cnws = mean_absolute_error_new(gbvs_base_cnws_saliency, new_objects_mask)\n",
    "                # calculate all spectralh64w64 combination errors\n",
    "                mae_new_spectralh64w64_base_pwp, mae_im_spectralh64w64_base_pwp = mean_absolute_error_new(spectralh64w64_base_pwp_saliency, new_objects_mask)\n",
    "                mae_new_spectralh64w64_base_ns, mae_im_spectralh64w64_base_ns = mean_absolute_error_new(spectralh64w64_base_ns_saliency, new_objects_mask)\n",
    "                mae_new_spectralh64w64_base_nm, mae_im_spectralh64w64_base_nm = mean_absolute_error_new(spectralh64w64_base_nm_saliency, new_objects_mask)\n",
    "                mae_new_spectralh64w64_base_cnws, mae_im_spectralh64w64_base_cnws = mean_absolute_error_new(spectralh64w64_base_cnws_saliency, new_objects_mask)\n",
    "                # calculate all spectralh8w128 combination errors\n",
    "                mae_new_spectralh128w128_base_pwp, mae_im_spectralh128w128_base_pwp = mean_absolute_error_new(spectralh128w128_base_pwp_saliency, new_objects_mask)\n",
    "                mae_new_spectralh128w128_base_ns, mae_im_spectralh128w128_base_ns = mean_absolute_error_new(spectralh128w128_base_ns_saliency, new_objects_mask)\n",
    "                mae_new_spectralh128w128_base_nm, mae_im_spectralh128w128_base_nm = mean_absolute_error_new(spectralh128w128_base_nm_saliency, new_objects_mask)\n",
    "                mae_new_spectralh128w128_base_cnws, mae_im_spectralh128w128_base_cnws = mean_absolute_error_new(spectralh128w128_base_cnws_saliency, new_objects_mask)\n",
    "                # calculate all spectralh8w64 combination errors\n",
    "                mae_new_spectralh8w64_base_pwp, mae_im_spectralh8w64_base_pwp = mean_absolute_error_new(spectralh8w64_base_pwp_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w64_base_ns, mae_im_spectralh8w64_base_ns = mean_absolute_error_new(spectralh8w64_base_ns_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w64_base_nm, mae_im_spectralh8w64_base_nm = mean_absolute_error_new(spectralh8w64_base_nm_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w64_base_cnws, mae_im_spectralh8w64_base_cnws = mean_absolute_error_new(spectralh8w64_base_cnws_saliency, new_objects_mask)\n",
    "                # calculate all spectralh8w128 combination errors\n",
    "                mae_new_spectralh8w128_base_pwp, mae_im_spectralh8w128_base_pwp = mean_absolute_error_new(spectralh8w128_base_pwp_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w128_base_ns, mae_im_spectralh8w128_base_ns = mean_absolute_error_new(spectralh8w128_base_ns_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w128_base_nm, mae_im_spectralh8w128_base_nm = mean_absolute_error_new(spectralh8w128_base_nm_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w128_base_cnws, mae_im_spectralh8w128_base_cnws = mean_absolute_error_new(spectralh8w128_base_cnws_saliency, new_objects_mask)\n",
    "                # calculate all spectralh8w256 combination errors\n",
    "                mae_new_spectralh8w256_base_pwp, mae_im_spectralh8w256_base_pwp = mean_absolute_error_new(spectralh8w256_base_pwp_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w256_base_ns, mae_im_spectralh8w256_base_ns = mean_absolute_error_new(spectralh8w256_base_ns_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w256_base_nm, mae_im_spectralh8w256_base_nm = mean_absolute_error_new(spectralh8w256_base_nm_saliency, new_objects_mask)\n",
    "                mae_new_spectralh8w256_base_cnws, mae_im_spectralh8w256_base_cnws = mean_absolute_error_new(spectralh8w256_base_cnws_saliency, new_objects_mask)\n",
    "\n",
    "                # add all solo errors\n",
    "                base_stats.add_sample(mae_new_base, mae_im_base)\n",
    "                gbvs_stats.add_sample(mae_new_gbvs, mae_im_gbvs)\n",
    "                spectralh64w64_stats.add_sample(mae_new_spectralh64w64, mae_im_spectralh64w64)\n",
    "                spectralh128w128_stats.add_sample(mae_new_spectralh128w128, mae_im_spectralh128w128)\n",
    "                spectralh8w64_stats.add_sample(mae_new_spectralh8w64, mae_im_spectralh8w64)\n",
    "                spectralh8w128_stats.add_sample(mae_new_spectralh8w128, mae_im_spectralh8w128)\n",
    "                spectralh8w256_stats.add_sample(mae_new_spectralh8w256, mae_im_spectralh8w256)\n",
    "                # add gbvs combination errors\n",
    "                gbvs_baseline_pwp_stats.add_sample(mae_new_gbvs_base_pwp, mae_im_gbvs_base_pwp)\n",
    "                gbvs_baseline_ns_stats.add_sample(mae_new_gbvs_base_ns, mae_im_gbvs_base_ns)\n",
    "                gbvs_baseline_nm_stats.add_sample(mae_new_gbvs_base_nm, mae_im_gbvs_base_nm)\n",
    "                gbvs_baseline_cnws_stats.add_sample(mae_new_gbvs_base_cnws, mae_im_gbvs_base_cnws)\n",
    "                # add spectral h64w64 combination errors\n",
    "                spectralh64w64_baseline_pwp_stats.add_sample(mae_new_spectralh64w64_base_pwp, mae_im_spectralh64w64_base_pwp)\n",
    "                spectralh64w64_baseline_ns_stats.add_sample(mae_new_spectralh64w64_base_ns, mae_im_spectralh64w64_base_ns)\n",
    "                spectralh64w64_baseline_nm_stats.add_sample(mae_new_spectralh64w64_base_nm, mae_im_spectralh64w64_base_nm)\n",
    "                spectralh64w64_baseline_cnws_stats.add_sample(mae_new_spectralh64w64_base_cnws, mae_im_spectralh64w64_base_cnws)\n",
    "                # add spectral h128w128 combination errors\n",
    "                spectralh128w128_baseline_pwp_stats.add_sample(mae_new_spectralh128w128_base_pwp, mae_im_spectralh128w128_base_pwp)\n",
    "                spectralh128w128_baseline_ns_stats.add_sample(mae_new_spectralh128w128_base_ns, mae_im_spectralh128w128_base_ns)\n",
    "                spectralh128w128_baseline_nm_stats.add_sample(mae_new_spectralh128w128_base_nm, mae_im_spectralh128w128_base_nm)\n",
    "                spectralh128w128_baseline_cnws_stats.add_sample(mae_new_spectralh128w128_base_cnws, mae_im_spectralh128w128_base_cnws)\n",
    "                # add spectral h8w64 combination errors\n",
    "                spectralh8w64_baseline_pwp_stats.add_sample(mae_new_spectralh8w64_base_pwp, mae_im_spectralh8w64_base_pwp)\n",
    "                spectralh8w64_baseline_ns_stats.add_sample(mae_new_spectralh8w64_base_ns, mae_im_spectralh8w64_base_ns)\n",
    "                spectralh8w64_baseline_nm_stats.add_sample(mae_new_spectralh8w64_base_nm, mae_im_spectralh8w64_base_nm)\n",
    "                spectralh8w64_baseline_cnws_stats.add_sample(mae_new_spectralh8w64_base_cnws, mae_im_spectralh8w64_base_cnws)\n",
    "                # add spectral h8w128 combination errors\n",
    "                spectralh8w128_baseline_pwp_stats.add_sample(mae_new_spectralh8w128_base_pwp, mae_im_spectralh8w128_base_pwp)\n",
    "                spectralh8w128_baseline_ns_stats.add_sample(mae_new_spectralh8w128_base_ns, mae_im_spectralh8w128_base_ns)\n",
    "                spectralh8w128_baseline_nm_stats.add_sample(mae_new_spectralh8w128_base_nm, mae_im_spectralh8w128_base_nm)\n",
    "                spectralh8w128_baseline_cnws_stats.add_sample(mae_new_spectralh8w128_base_cnws, mae_im_spectralh8w128_base_cnws)\n",
    "                # add spectral h8w256 combination errors\n",
    "                spectralh8w256_baseline_pwp_stats.add_sample(mae_new_spectralh8w256_base_pwp, mae_im_spectralh8w256_base_pwp)\n",
    "                spectralh8w256_baseline_ns_stats.add_sample(mae_new_spectralh8w256_base_ns, mae_im_spectralh8w256_base_ns)\n",
    "                spectralh8w256_baseline_nm_stats.add_sample(mae_new_spectralh8w256_base_nm, mae_im_spectralh8w256_base_nm)\n",
    "                spectralh8w256_baseline_cnws_stats.add_sample(mae_new_spectralh8w256_base_cnws, mae_im_spectralh8w256_base_cnws)\n",
    "                \n",
    "                if debug_single:\n",
    "                    debug_im_and_saliency(current_im_box, base_saliency, \"base_saliency\", mae_new_base, mae_im_base)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_saliency, \"gbvs_saliency\", mae_new_gbvs, mae_im_gbvs)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh64w64_saliency, \"spectral_residual_h64w64\", mae_new_spectralh64w64, mae_im_spectralh64w64)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh128w128_saliency, \"spectral_residual_h128w128\", mae_new_spectralh128w128, mae_im_spectralh128w128)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w64_saliency, \"spectral_residual_h8w64\", mae_new_spectralh8w64, mae_im_spectralh8w64)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_saliency, \"spectral_residual_h8w128\", mae_new_spectralh8w128, mae_im_spectralh8w128)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w256_saliency, \"spectral_residual_h8w256\", mae_new_spectralh8w256, mae_im_spectralh8w256)\n",
    "                    # # display gbvs combinations\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_pwp_saliency, \"gbvs_base_pwp_saliency\", mae_new_gbvs_base_pwp, mae_im_gbvs_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_ns_saliency, \"gbvs_base_ns_saliency\", mae_new_gbvs_base_ns, mae_im_gbvs_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_nm_saliency, \"gbvs_base_nm_saliency\", mae_new_gbvs_base_nm, mae_im_gbvs_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_cnws_saliency, \"gbvs_base_cnws_saliency\", mae_new_gbvs_base_cnws, mae_im_gbvs_base_cnws)\n",
    "                    # # display spectral combinations\n",
    "                    debug_im_and_saliency(current_im_box, spectralh64w64_base_pwp_saliency, \"spectralh64w64_base_pwp_saliency\", mae_new_spectralh64w64_base_pwp, mae_im_spectralh64w64_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh64w64_base_ns_saliency, \"spectralh64w64_base_ns_saliency\", mae_new_spectralh64w64_base_ns, mae_im_spectralh64w64_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh64w64_base_nm_saliency, \"spectralh64w64_base_nm_saliency\", mae_new_spectralh64w64_base_nm, mae_im_spectralh64w64_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh64w64_base_cnws_saliency, \"spectralh64w64_base_cnws_saliency\", mae_new_spectralh64w64_base_cnws, mae_im_spectralh64w64_base_cnws)\n",
    "                    # display spectral combinations\n",
    "                    debug_im_and_saliency(current_im_box, spectralh128w128_base_pwp_saliency, \"spectralh128w128_base_pwp_saliency\", mae_new_spectralh128w128_base_pwp, mae_im_spectralh128w128_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh128w128_base_ns_saliency, \"spectralh128w128_base_ns_saliency\", mae_new_spectralh128w128_base_ns, mae_im_spectralh128w128_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh128w128_base_nm_saliency, \"spectralh128w128_base_nm_saliency\", mae_new_spectralh128w128_base_nm, mae_im_spectralh128w128_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh128w128_base_cnws_saliency, \"spectralh128w128_base_cnws_saliency\", mae_new_spectralh128w128_base_cnws, mae_im_spectralh128w128_base_cnws)\n",
    "                    # display spectral combinations\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w64_base_pwp_saliency, \"spectralh8w64_base_pwp_saliency\", mae_new_spectralh8w64_base_pwp, mae_im_spectralh8w64_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w64_base_ns_saliency, \"spectralh8w64_base_ns_saliency\", mae_new_spectralh8w64_base_ns, mae_im_spectralh8w64_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w64_base_nm_saliency, \"spectralh8w64_base_nm_saliency\", mae_new_spectralh8w64_base_nm, mae_im_spectralh8w64_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w64_base_cnws_saliency, \"spectralh8w64_base_cnws_saliency\", mae_new_spectralh8w64_base_cnws, mae_im_spectralh8w64_base_cnws)\n",
    "                    # display spectral combinations\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_pwp_saliency, \"spectralh8w128_base_pwp_saliency\", mae_new_spectralh8w128_base_pwp, mae_im_spectralh8w128_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_ns_saliency, \"spectralh8w128_base_ns_saliency\", mae_new_spectralh8w128_base_ns, mae_im_spectralh8w128_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_nm_saliency, \"spectralh8w128_base_nm_saliency\", mae_new_spectralh8w128_base_nm, mae_im_spectralh8w128_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_cnws_saliency, \"spectralh8w128_base_cnws_saliency\", mae_new_spectralh8w128_base_cnws, mae_im_spectralh8w128_base_cnws)\n",
    "                    # display spectral combinations\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w256_base_pwp_saliency, \"spectralh8w256_base_pwp_saliency\", mae_new_spectralh8w256_base_pwp, mae_im_spectralh8w256_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w256_base_ns_saliency, \"spectralh8w256_base_ns_saliency\", mae_new_spectralh8w256_base_ns, mae_im_spectralh8w256_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w256_base_nm_saliency, \"spectralh8w256_base_nm_saliency\", mae_new_spectralh8w256_base_nm, mae_im_spectralh8w256_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w256_base_cnws_saliency, \"spectralh8w256_base_cnws_saliency\", mae_new_spectralh8w256_base_cnws, mae_im_spectralh8w256_base_cnws)\n",
    "\n",
    "                if debug:\n",
    "                    new_objects_mask = cv2.cvtColor(new_objects_mask, cv2.COLOR_GRAY2BGR)\n",
    "                    # debug all images\n",
    "                    indices_already_tracked_objects = np.where(already_tracked_objects_mask == 255)\n",
    "                    already_tracked_objects_mask = cv2.cvtColor(already_tracked_objects_mask, cv2.COLOR_GRAY2BGR)\n",
    "                    if len(indices_already_tracked_objects[0]) > 0:\n",
    "                        already_tracked_objects_mask[indices_already_tracked_objects[0], indices_already_tracked_objects[1]] = (0, 0, 255)\n",
    "                    # already_tracked_objects_mask = cv2.cvtColor(already_tracked_objects_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                    display_im = cv2.addWeighted(current_im, 0.7, new_objects_mask, 0.3, 0.0)\n",
    "                    display_im = cv2.addWeighted(display_im, 0.7, already_tracked_objects_mask, 0.3, 0.0)\n",
    "                    cv2.imshow(\"im, new and tracked objects\", display_im)\n",
    "                    cv2.imshow(\"already tracked objects\", already_tracked_objects_mask)\n",
    "                    if im_contains_unique_instance_and_is_not_first_frame:\n",
    "                        key = cv2.waitKey(0)\n",
    "                    else:\n",
    "                        key = cv2.waitKey(1)\n",
    "\n",
    "                    if key == 27:\n",
    "                        break_all = True\n",
    "                        break\n",
    "                    \n",
    "        if break_all:\n",
    "            break\n",
    "        \n",
    "        if current_sd_rec['next'] == '':\n",
    "            has_more_frames = False\n",
    "        else:\n",
    "            current_sd_rec = nusc.get('sample_data', current_sd_rec['next'])\n",
    "        \n",
    "    if break_all:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE_NEW_ONLY -> bounding box of new object compared to saliency map cutout\n",
    "# MAE_WHOLE_IMAGE -> bounding box of new object and whole image compared to whole saliency map\n",
    "# MAE_TRACKED_OBJECTS_ONLY -> bounding box of already tracked objects compared to saliency map cutout\n",
    "# MAE_TRACKED_OBJECTS_WHOLE_IMAGE -> bounding box of already tracked objects and whole image compared to saliency map\n",
    "\n",
    "\n",
    "# try out spectralh8w256 for splitting birth areas\n",
    "# protoobjects see spectral residual paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pickle objects\n",
    "import pickle\n",
    "base_pickle_path = \"/data/datasets/nuscenes_results/eval_keyframes_only_new1\"\n",
    "\n",
    "def save_stats(stats, name):\n",
    "    with open(base_pickle_path + os.sep + name + \".pkl\", 'wb') as outp:\n",
    "        pickle.dump(stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_stats(timestamps, \"timestamps\")\n",
    "save_stats(scene_idxs, \"scene_idxs\")\n",
    "save_stats(image_filenames, \"image_filenames\")\n",
    "save_stats(categories, \"categories\")\n",
    "# single\n",
    "save_stats(base_stats, \"base_stats\")\n",
    "save_stats(gbvs_stats, \"gbvs_stats\")\n",
    "save_stats(spectralh64w64_stats, \"spectralh64w64_stats\")\n",
    "save_stats(spectralh128w128_stats, \"spectralh128w128_stats\")\n",
    "save_stats(spectralh8w64_stats, \"spectralh8w64_stats\")\n",
    "save_stats(spectralh8w128_stats, \"spectralh8w128_stats\")\n",
    "save_stats(spectralh8w256_stats, \"spectralh8w256_stats\")\n",
    "# gbvs combi\n",
    "save_stats(gbvs_baseline_pwp_stats, \"gbvs_baseline_pwp_stats\")\n",
    "save_stats(gbvs_baseline_ns_stats, \"gbvs_baseline_ns_stats\")\n",
    "save_stats(gbvs_baseline_nm_stats, \"gbvs_baseline_nm_stats\")\n",
    "save_stats(gbvs_baseline_cnws_stats, \"gbvs_baseline_cnws_stats\")\n",
    "# spectralh64w64 combi\n",
    "save_stats(spectralh64w64_baseline_pwp_stats, \"spectralh64w64_baseline_pwp_stats\")\n",
    "save_stats(spectralh64w64_baseline_ns_stats, \"spectralh64w64_baseline_ns_stats\")\n",
    "save_stats(spectralh64w64_baseline_nm_stats, \"spectralh64w64_baseline_nm_stats\")\n",
    "save_stats(spectralh64w64_baseline_cnws_stats, \"spectralh64w64_baseline_cnws_stats\")\n",
    "# spectralh128w128 combi\n",
    "save_stats(spectralh128w128_baseline_pwp_stats, \"spectralh128w128_baseline_pwp_stats\")\n",
    "save_stats(spectralh128w128_baseline_ns_stats, \"spectralh128w128_baseline_ns_stats\")\n",
    "save_stats(spectralh128w128_baseline_nm_stats, \"spectralh128w128_baseline_nm_stats\")\n",
    "save_stats(spectralh128w128_baseline_cnws_stats, \"spectralh128w128_baseline_cnws_stats\")\n",
    "# spectralh8w64 combi\n",
    "save_stats(spectralh8w64_baseline_pwp_stats, \"spectralh8w64_baseline_pwp_stats\")\n",
    "save_stats(spectralh8w64_baseline_ns_stats, \"spectralh8w64_baseline_ns_stats\")\n",
    "save_stats(spectralh8w64_baseline_nm_stats, \"spectralh8w64_baseline_nm_stats\")\n",
    "save_stats(spectralh8w64_baseline_cnws_stats, \"spectralh8w64_baseline_cnws_stats\")\n",
    "# spectralh8w128 combi\n",
    "save_stats(spectralh8w128_baseline_pwp_stats, \"spectralh8w128_baseline_pwp_stats\")\n",
    "save_stats(spectralh8w128_baseline_ns_stats, \"spectralh8w128_baseline_ns_stats\")\n",
    "save_stats(spectralh8w128_baseline_nm_stats, \"spectralh8w128_baseline_nm_stats\")\n",
    "save_stats(spectralh8w128_baseline_cnws_stats, \"spectralh8w128_baseline_cnws_stats\")\n",
    "# spectralh8w256 combi\n",
    "save_stats(spectralh8w256_baseline_pwp_stats, \"spectralh8w256_baseline_pwp_stats\")\n",
    "save_stats(spectralh8w256_baseline_ns_stats, \"spectralh8w256_baseline_ns_stats\")\n",
    "save_stats(spectralh8w256_baseline_nm_stats, \"spectralh8w256_baseline_nm_stats\")\n",
    "save_stats(spectralh8w256_baseline_cnws_stats, \"spectralh8w256_baseline_cnws_stats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base                           n: 2  mae_new 0.4035987381  std_new  0.0248553979  mae_tracked  0.0844349499  std_tracked  0.0016639734\n",
      "gbvs                           n: 2  mae_new 0.8689432949  std_new  0.0299471913  mae_tracked  0.1686851563  std_tracked  0.0006110894\n",
      "spectralh64w64                 n: 2  mae_new 0.8674382029  std_new  0.0153462375  mae_tracked  0.0999057726  std_tracked  0.0042432834\n",
      "spectralh128w128               n: 2  mae_new 0.9235159120  std_new  0.0063390437  mae_tracked  0.0575329400  std_tracked  0.0062305474\n",
      "spectralh8w64                  n: 2  mae_new 0.9181954066  std_new  0.0074269175  mae_tracked  0.1247022786  std_tracked  0.0096881402\n",
      "spectralh8w128                 n: 2  mae_new 0.9181954066  std_new  0.0074269175  mae_tracked  0.1247022786  std_tracked  0.0096881402\n",
      "spectralh8w256                 n: 2  mae_new 0.9320089910  std_new  0.0264112910  mae_tracked  0.1106496935  std_tracked  0.0279455431\n",
      "gbvs_baseline_pwp              n: 2  mae_new 0.7101808489  std_new  0.0936127431  mae_tracked  0.0586337363  std_tracked  0.0018959788\n",
      "gbvs_baseline_ns               n: 2  mae_new 0.3145641356  std_new  0.0292772825  mae_tracked  0.2285581247  std_tracked  0.0137010695\n",
      "gbvs_baseline_nm               n: 2  mae_new 0.4035141870  std_new  0.0249399490  mae_tracked  0.2039144617  std_tracked  0.0066009910\n",
      "gbvs_baseline_cnws             n: 2  mae_new 0.4429354096  std_new  0.0619770613  mae_tracked  0.1734546731  std_tracked  0.0139695526\n",
      "spectralh64w64_baseline_pwp    n: 2  mae_new 0.5476468211  std_new  0.0095421326  mae_tracked  0.0332837707  std_tracked  0.0064705376\n",
      "spectralh64w64_baseline_ns     n: 2  mae_new 0.3497753342  std_new  0.0379858866  mae_tracked  0.1552230831  std_tracked  0.0092071909\n",
      "spectralh64w64_baseline_nm     n: 2  mae_new 0.4035979294  std_new  0.0248562066  mae_tracked  0.1553156055  std_tracked  0.0079307656\n",
      "spectralh64w64_baseline_cnws   n: 2  mae_new 0.3841984285  std_new  0.0320494377  mae_tracked  0.1279641809  std_tracked  0.0112952436\n",
      "spectralh128w128_baseline_pwp_stats n: 2  mae_new 0.6897158354  std_new  0.0128664634  mae_tracked  0.0244676343  std_tracked  0.0006278031\n",
      "spectralh128w128_baseline_ns_stats n: 2  mae_new 0.3540861542  std_new  0.0385167997  mae_tracked  0.1255469443  std_tracked  0.0007267044\n",
      "spectralh128w128_baseline_nm_stats n: 2  mae_new 0.4035987381  std_new  0.0248553979  mae_tracked  0.1196383900  std_tracked  0.0006958601\n",
      "spectralh128w128_baseline_cnws_stats n: 2  mae_new 0.3827869302  std_new  0.0020307013  mae_tracked  0.1105703760  std_tracked  0.0056110235\n",
      "spectralh8w64_baseline_pwp_stats n: 2  mae_new 0.6904308847  std_new  0.0450979939  mae_tracked  0.0494317473  std_tracked  0.0047615558\n",
      "spectralh8w64_baseline_ns_stats n: 2  mae_new 0.4012242851  std_new  0.0313431253  mae_tracked  0.1752672859  std_tracked  0.0098619560\n",
      "spectralh8w64_baseline_nm_stats n: 2  mae_new 0.4035987381  std_new  0.0248553979  mae_tracked  0.1734587199  std_tracked  0.0131046864\n",
      "spectralh8w64_baseline_cnws_stats n: 2  mae_new 0.4505493119  std_new  0.0427604365  mae_tracked  0.1512060222  std_tracked  0.0034879217\n",
      "spectralh8w128_baseline_pwp_stats n: 2  mae_new 0.6904308847  std_new  0.0450979939  mae_tracked  0.0494317473  std_tracked  0.0047615558\n",
      "spectralh8w128_baseline_ns_stats n: 2  mae_new 0.4012242851  std_new  0.0313431253  mae_tracked  0.1752672859  std_tracked  0.0098619560\n",
      "spectralh8w128_baseline_nm_stats n: 2  mae_new 0.4035987381  std_new  0.0248553979  mae_tracked  0.1734587199  std_tracked  0.0131046864\n",
      "spectralh8w128_baseline_cnws_stats n: 2  mae_new 0.4505493119  std_new  0.0427604365  mae_tracked  0.1512060222  std_tracked  0.0034879217\n",
      "spectralh8w256_baseline_pwp_stats n: 2  mae_new 0.7965570853  std_new  0.0065368597  mae_tracked  0.0394576003  std_tracked  0.0035456363\n",
      "spectralh8w256_baseline_ns_stats n: 2  mae_new 0.4294071584  std_new  0.0431007531  mae_tracked  0.1568103577  std_tracked  0.0182971405\n",
      "spectralh8w256_baseline_nm_stats n: 2  mae_new 0.4035987381  std_new  0.0248553979  mae_tracked  0.1618238834  std_tracked  0.0287078006\n",
      "spectralh8w256_baseline_cnws_stats n: 2  mae_new 0.5017261345  std_new  0.0683973908  mae_tracked  0.1298340302  std_tracked  0.0035371110\n"
     ]
    }
   ],
   "source": [
    "# print stats\n",
    "base_stats.print_stats()\n",
    "gbvs_stats.print_stats()\n",
    "spectralh64w64_stats.print_stats()\n",
    "spectralh128w128_stats.print_stats()\n",
    "spectralh8w64_stats.print_stats()\n",
    "spectralh8w128_stats.print_stats()\n",
    "spectralh8w256_stats.print_stats()\n",
    "gbvs_baseline_pwp_stats.print_stats()\n",
    "gbvs_baseline_ns_stats.print_stats()\n",
    "gbvs_baseline_nm_stats.print_stats()\n",
    "gbvs_baseline_cnws_stats.print_stats()\n",
    "spectralh64w64_baseline_pwp_stats.print_stats()\n",
    "spectralh64w64_baseline_ns_stats.print_stats()\n",
    "spectralh64w64_baseline_nm_stats.print_stats()\n",
    "spectralh64w64_baseline_cnws_stats.print_stats()\n",
    "spectralh128w128_baseline_pwp_stats.print_stats()\n",
    "spectralh128w128_baseline_ns_stats.print_stats()\n",
    "spectralh128w128_baseline_nm_stats.print_stats()\n",
    "spectralh128w128_baseline_cnws_stats.print_stats()\n",
    "spectralh8w64_baseline_pwp_stats.print_stats()\n",
    "spectralh8w64_baseline_ns_stats.print_stats()\n",
    "spectralh8w64_baseline_nm_stats.print_stats()\n",
    "spectralh8w64_baseline_cnws_stats.print_stats()\n",
    "spectralh8w128_baseline_pwp_stats.print_stats()\n",
    "spectralh8w128_baseline_ns_stats.print_stats()\n",
    "spectralh8w128_baseline_nm_stats.print_stats()\n",
    "spectralh8w128_baseline_cnws_stats.print_stats()\n",
    "spectralh8w256_baseline_pwp_stats.print_stats()\n",
    "spectralh8w256_baseline_ns_stats.print_stats()\n",
    "spectralh8w256_baseline_nm_stats.print_stats()\n",
    "spectralh8w256_baseline_cnws_stats.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_stats_nicely(stats: SaliencyEvalStats):\n",
    "#     print(\"{:30} n: {:d}  mae_new {:0.10f}  std_new  {:0.10f}  mae_tracked  {:0.10f}  std_tracked  {:0.10f}\".format(\n",
    "#         stats.name, stats.n, np.mean(np.asarray(stats.mean_absolute_error_new_objects)), np.std(np.asarray(stats.mean_absolute_error_new_objects)),\n",
    "#         np.mean(np.asarray(stats.mean_absolute_error_already_tracked_objects)), np.std(np.asarray(stats.mean_absolute_error_already_tracked_objects))))\n",
    "\n",
    "# print_stats_nicely(base_stats)\n",
    "# print_stats_nicely(gbvs_stats)\n",
    "# print_stats_nicely(spectralh8w128_stats)\n",
    "# print_stats_nicely(spectralh64w64_stats)\n",
    "# print_stats_nicely(spectralh128w128_stats)\n",
    "# print_stats_nicely(gbvs_baseline_pwp_stats)\n",
    "# print_stats_nicely(gbvs_baseline_ns_stats)\n",
    "# print_stats_nicely(gbvs_baseline_nm_stats)\n",
    "# print_stats_nicely(gbvs_baseline_cnws_stats)\n",
    "# print_stats_nicely(spectralh64w64_baseline_pwp_stats)\n",
    "# print_stats_nicely(spectralh64w64_baseline_ns_stats)\n",
    "# print_stats_nicely(spectralh64w64_baseline_nm_stats)\n",
    "# print_stats_nicely(spectralh64w64_baseline_cnws_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# # TODO save MAE stats for every sample to find outliers and investigate distribution of data\n",
    "\n",
    "# width = 1600\n",
    "# height = 900\n",
    "# display_image = True\n",
    "# save_images = True\n",
    "# # get all images and birth masks\n",
    "# base_data_path = \"/data/datasets/nuscenes_results/birth_frames_and_masks\"\n",
    "\n",
    "# image_filenames = sorted(glob.glob(base_data_path + os.sep + \"*image*\", recursive=False))\n",
    "# mask_filenames = sorted(glob.glob(base_data_path + os.sep + \"*mask*\", recursive=False))\n",
    "# gbvs_filenames = sorted(glob.glob(base_data_path + os.sep + \"*gbvs*\", recursive=False))\n",
    "\n",
    "# assert(len(image_filenames) == len(mask_filenames) and len(gbvs_filenames) == len(mask_filenames))\n",
    "\n",
    "# # create spectral residual object\n",
    "# spectral_residual_height = 8\n",
    "# spectral_residual_width = 256\n",
    "# spectral_residual = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "# spectral_residual.setImageHeight(spectral_residual_height)\n",
    "# spectral_residual.setImageWidth(spectral_residual_width)\n",
    "\n",
    "# # prepare object birth histogram\n",
    "# baseline_histogram_filename = \"/data/datasets/nuscenes_results/object_birth_heatmaps/birth_heatmap_0849.npy\"\n",
    "# baseline_birth_histogram = np.load(baseline_histogram_filename)\n",
    "# baseline_birth_histogram = normalize(baseline_birth_histogram)\n",
    "\n",
    "# # saliency maps to be evaluated\n",
    "# saliency_types = ['baseline', 'gbvs', 'spectral_residual_h8w256']\n",
    "# # Combinations?\n",
    "# saliency_results = {}\n",
    "# for saliency_type in saliency_types:\n",
    "#     saliency_results[saliency_type] = RunningStats()\n",
    "\n",
    "# for image_filename, mask_filename, gbvs_filename in zip(image_filenames, mask_filenames, gbvs_filenames):\n",
    "#     img = cv2.imread(image_filename)\n",
    "#     mask = cv2.imread(mask_filename, flags=cv2.IMREAD_GRAYSCALE)\n",
    "#     gbvs = cv2.imread(gbvs_filename, flags=(cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH))\n",
    "#     (success, saliency_spectral_residual) = spectral_residual.computeSaliency(img)\n",
    "#     if not success:\n",
    "#         print(\"Couldn't create saliency for \", image_filename)\n",
    "#         continue\n",
    "    \n",
    "#     img_and_gbvs = overlay_image_and_saliency_as_heatmap(img, gbvs, 1.0)\n",
    "#     img_and_baseline = overlay_image_and_saliency_as_heatmap(img, baseline_birth_histogram, 1.0)\n",
    "#     # saliency_spectral_residual = cv2.cvtColor((saliency_spectral_residual*255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "#     img_and_spectral_residual = overlay_image_and_saliency_as_heatmap(img, normalize(saliency_spectral_residual), 1.0)\n",
    "\n",
    "#     mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "#     img_and_gbvs_and_mask = cv2.addWeighted(img_and_gbvs, 0.7, mask_bgr, 0.3, 0.0)\n",
    "#     img_and_baseline_and_mask = cv2.addWeighted(img_and_baseline, 0.7, mask_bgr, 0.3, 0.0)\n",
    "#     img_and_spectral_residual_and_mask = cv2.addWeighted(img_and_spectral_residual, 0.7, mask_bgr, 0.3, 0.0)\n",
    "\n",
    "#     mae_baseline = mean_absolute_error(baseline_birth_histogram, mask)\n",
    "#     mae_gbvs = mean_absolute_error(gbvs, mask)\n",
    "#     mae_spectral_residual = mean_absolute_error(saliency_spectral_residual, mask)#\n",
    "\n",
    "#     saliency_results['baseline'].add_sample(mae_baseline)\n",
    "#     saliency_results['gbvs'].add_sample(mae_gbvs)\n",
    "#     saliency_results['spectral_residual_h8w256'].add_sample(mae_spectral_residual)\n",
    "\n",
    "#     mae_baseline = mean_absolute_error(baseline_birth_histogram, mask)\n",
    "#     cv2_put_multi_line_text(img_and_gbvs_and_mask, str(mae_gbvs), (width/2, height/2), (0, 0, 0))\n",
    "#     cv2_put_multi_line_text(img_and_baseline_and_mask, str(mae_baseline), (width/2, height/2), (0, 0, 0))\n",
    "#     cv2.imshow(\"input img and gbvs overlayed\", img_and_spectral_residual_and_mask)\n",
    "#     key = cv2.waitKey(0)\n",
    "#     print(\"MEAN base {} gbvs {} spectral {}\".format(\n",
    "#         saliency_results['baseline'].mean(),\n",
    "#         saliency_results['gbvs'].mean(),\n",
    "#         saliency_results['spectral_residual_h8w256'].mean()))\n",
    "#     print(\"STD  base {} gbvs {} spectral {}\\n\".format(\n",
    "#         saliency_results['baseline'].standard_deviation(),\n",
    "#         saliency_results['gbvs'].standard_deviation(),\n",
    "#         saliency_results['spectral_residual_h8w256'].standard_deviation()))\n",
    "#     if key == 27:  # if ESC is pressed, exit.\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convex mask code\n",
    "# sample_record = nusc.get('sample', current_sd_rec['sample_token'])\n",
    "# sample_token = current_sd_rec['sample_token']\n",
    "# pointsensor_token = sample_record['data'][pointsensor_channel]\n",
    "# camera_token = sample_record['data'][camera_channel]\n",
    "# lidar_points_inside_box, coloring = map_pointcloud_to_box(nusc, sample_token, pointsensor_token, camera_token, box)\n",
    "# lidar_points_inside_box = lidar_points_inside_box[:2, :]\n",
    "# convex_mask, convex_hull_valid, hull_points = convex_mask_and_hull_points_from_lidar_points(\n",
    "#     box_mask, lidar_points_inside_box, width, height)\n",
    "# indices_contour = np.where(convex_mask == 255)\n",
    "# convex_mask = cv2.cvtColor(convex_mask, cv2.COLOR_GRAY2BGR)\n",
    "# convex_mask[indices_contour[0], indices_contour[1]] = (0, 0, 255)\n",
    "# final_im = cv2.addWeighted(final_im, 0.7, convex_mask, 0.3, 0.0)\n",
    "# cv2_put_multi_line_text(final_im, str(convex_hull_valid), (width/2, height/2), color=(255, 0, 0))\n",
    "# for p in lidar_points_inside_box.T:\n",
    "#     cv2.drawMarker(final_im, (p[0].astype(int), p[1].astype(int)), color=(\n",
    "#         0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=20)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "318541fadf8f6473a5ac117b97e36e34b5dead6f3ba376595b86165903d1f35a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('nuscenes_local': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
