{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-panoptic...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "34149 panoptic,\n",
      "Done loading in 22.462 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.9 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import subprocess\n",
    "from typing import Tuple, List\n",
    "import os.path as osp\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import points_in_box\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes.utils.geometry_utils import BoxVisibility\n",
    "from nuscenes.panoptic.panoptic_utils import paint_panop_points_label\n",
    "from nuscenes.lidarseg.lidarseg_utils import paint_points_label\n",
    "from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud\n",
    "\n",
    "import numpy as np\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "import cv2\n",
    "\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot='/data/datasets/nuscenes', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(smap):\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    # This operator uses the global\n",
    "    # maximum of each map to obtain a dynamic range between\n",
    "    # 0 and 1\n",
    "   \n",
    "    if smap.min() <0 :\n",
    "        sal_map=smap+np.abs(smap.min())\n",
    "    else:\n",
    "        sal_map=smap\n",
    "        \n",
    "    sal_map=sal_map/sal_map.max()\n",
    "    \n",
    "    return sal_map\n",
    "    \n",
    "def normalized_and_sum(smap1, smap2):\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    \n",
    "    n_smap1 = normalize(smap1)\n",
    "    n_smap2 = normalize(smap2)\n",
    "    \n",
    "    raw_sum_map = n_smap1 + n_smap2\n",
    "    \n",
    "    outmap = normalize(raw_sum_map)\n",
    "    \n",
    "    return outmap\n",
    "\n",
    "\n",
    "def normalized_and_maximum(smap1, smap2):\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    \n",
    "    n_smap1 = normalize(smap1)\n",
    "    n_smap2 = normalize(smap2)\n",
    "    \n",
    "    outmap = np.maximum(n_smap1, n_smap2)\n",
    "\n",
    "    return outmap\n",
    "    \n",
    "def simple_pixel_wise_product(map_a, map_b):\n",
    "    comb_map=map_a*map_b\n",
    "    \n",
    "    #normalize\n",
    "    comb_map=normalize(comb_map)\n",
    "    \n",
    "    return comb_map\n",
    "\n",
    "def coherent_normalization_with_weighted_sum(smap1, smap2, alpha=0.5, beta=1.0):\n",
    "    # REF: https://ieeexplore.ieee.org/document/6605606\n",
    "    \n",
    "    outmap = (1 - alpha)*smap1 + alpha*smap2 + beta*( smap1*smap2)\n",
    "    \n",
    "    return outmap\n",
    "\n",
    "def average_local_maxima(smap):\n",
    "    # t = time.time()\n",
    "    local_max_loc=peak_local_max(smap)\n",
    "    # print(\"peak local max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    #all local maxima\n",
    "    all_local_max = smap[local_max_loc[:,0], local_max_loc[:,1]]\n",
    "    # print(\"all local max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    # drop the absolute maximum\n",
    "    other_local_max = np.delete(all_local_max, np.argmax(all_local_max))\n",
    "    # print(\"drop the absolute maximum max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    mean = np.mean(other_local_max)\n",
    "    # print(\"mean in average local maxima {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    return mean\n",
    "\n",
    "def global_nonlinear_amplification(smap1, smap2):\n",
    "    # original REF: http://authors.library.caltech.edu/71459/1/161_1.pdf\n",
    "    # REF: https://ieeexplore.ieee.org/abstract/document/5651381\n",
    "    # REF for local maxima: https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_peak_local_max.html\n",
    "    \n",
    "    # t = time.time()\n",
    "    n_smap1 = normalize(smap1)\n",
    "    # print(\"normalize 1 maximum max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    n_smap2 = normalize(smap2)\n",
    "    # print(\"normalize 2 maximum max {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    # follows notation of Le Meur \n",
    "    M_1 = n_smap1.max()\n",
    "    # print(\"max of normalized saliency 1 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    m_1 = average_local_maxima (smap1)\n",
    "    # print(\"average local maxima total 1 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "\n",
    "    M_2 = n_smap2.max()\n",
    "    # print(\"max of normalized saliency 2 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    m_2 = average_local_maxima (smap2)\n",
    "    # print(\"average local maxima total 2 {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "    \n",
    "    outmap = ( n_smap1 * (M_1-m_1)**2 ) + ( n_smap2 * (M_2-m_2 )**2 )\n",
    "    # print(\"outmap {:0.10f}\".format(time.time()-t))\n",
    "    # t = time.time()\n",
    "\n",
    "    return outmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "font_scale = 0.5\n",
    "thickness = 1\n",
    "\n",
    "\n",
    "def cv2_put_multi_line_text(im: np.ndarray,\n",
    "                            text: str,\n",
    "                            center: Tuple,\n",
    "                            color: Tuple,\n",
    "                            font=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                            font_scale=0.5,\n",
    "                            thickness=1) -> None:\n",
    "    text_size, _ = cv2.getTextSize(\"dummy_text\", font, font_scale, thickness)\n",
    "    line_height = text_size[1] + 5\n",
    "    y0 = int(center[1])\n",
    "    for i, text_line in enumerate(text.split('\\n')):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(im, text_line, (int(center[0]), y),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, font_scale, color[::-1], thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_pointcloud_to_box(nusc: NuScenes,\n",
    "                        sample_token: str,\n",
    "                        pointsensor_token: str,\n",
    "                        camera_token: str,\n",
    "                        box: Box,\n",
    "                        # point_sensor_channel: str ='LIDAR_TOP',\n",
    "                        # camera_channel: str = 'CAM_FRONT',\n",
    "                        min_dist: float = 1.0,\n",
    "                        render_intensity: bool = False,\n",
    "                        show_lidarseg: bool = False,\n",
    "                        filter_lidarseg_labels: List = None,\n",
    "                        lidarseg_preds_bin_path: str = None,\n",
    "                        show_panoptic: bool = False) -> Tuple:\n",
    "    \"\"\"\n",
    "    Given a point sensor (lidar/radar) token and camera sample_data token, load pointcloud and map it to the image\n",
    "    plane.\n",
    "    :param nusc: Active NuScenes object.\n",
    "    :param sample_token: Sample token.\n",
    "    :param pointsensor_token: Lidar/radar sample_data token.\n",
    "    :param camera_token: Camera sample_data token.\n",
    "    :param box: Box that we want map points to.\n",
    "    :param min_dist: Distance from the camera below which points are discarded.\n",
    "    :param render_intensity: Whether to render lidar intensity instead of point depth.\n",
    "    :param show_lidarseg: Whether to render lidar intensity instead of point depth.\n",
    "    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes. If None\n",
    "        or the list is empty, all classes will be displayed.\n",
    "    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation\n",
    "                                    predictions for the sample.\n",
    "    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set\n",
    "        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.\n",
    "        If show_lidarseg is True, show_panoptic will be set to False.\n",
    "    :return (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n",
    "    \"\"\"\n",
    "\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "    cam = nusc.get('sample_data', camera_token)\n",
    "    pcl_path = osp.join(nusc.dataroot, pointsensor['filename'])\n",
    "    if pointsensor['sensor_modality'] == 'lidar':\n",
    "        if show_lidarseg or show_panoptic:\n",
    "            gt_from = 'lidarseg' if show_lidarseg else 'panoptic'\n",
    "            assert hasattr(nusc.nusc, gt_from), f'Error: nuScenes-{gt_from} not installed!'\n",
    "\n",
    "            # Ensure that lidar pointcloud is from a keyframe.\n",
    "            assert pointsensor['is_key_frame'], \\\n",
    "                'Error: Only pointclouds which are keyframes have lidar segmentation labels. Rendering aborted.'\n",
    "\n",
    "            assert not render_intensity, 'Error: Invalid options selected. You can only select either ' \\\n",
    "                                            'render_intensity or show_lidarseg, not both.'\n",
    "\n",
    "        pc = LidarPointCloud.from_file(pcl_path)\n",
    "    else:\n",
    "        pc = RadarPointCloud.from_file(pcl_path)\n",
    "    # im = Image.open(osp.join(nusc.dataroot, cam['filename']))\n",
    "\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = pc.points[2, :]\n",
    "\n",
    "    if render_intensity:\n",
    "        assert pointsensor['sensor_modality'] == 'lidar', 'Error: Can only render intensity for lidar, ' \\\n",
    "                                                            'not %s!' % pointsensor['sensor_modality']\n",
    "        # Retrieve the color from the intensities.\n",
    "        # Performs arbitary scaling to achieve more visually pleasing results.\n",
    "        intensities = pc.points[3, :]\n",
    "        intensities = (intensities - np.min(intensities)) / (np.max(intensities) - np.min(intensities))\n",
    "        intensities = intensities ** 0.1\n",
    "        intensities = np.maximum(0, intensities - 0.5)\n",
    "        coloring = intensities\n",
    "    elif show_lidarseg or show_panoptic:\n",
    "        assert pointsensor['sensor_modality'] == 'lidar', 'Error: Can only render lidarseg labels for lidar, ' \\\n",
    "                                                            'not %s!' % pointsensor['sensor_modality']\n",
    "\n",
    "        gt_from = 'lidarseg' if show_lidarseg else 'panoptic'\n",
    "        semantic_table = getattr(nusc.nusc, gt_from)\n",
    "\n",
    "        if lidarseg_preds_bin_path:\n",
    "            sample_token = nusc.get('sample_data', pointsensor_token)['sample_token']\n",
    "            lidarseg_labels_filename = lidarseg_preds_bin_path\n",
    "            assert os.path.exists(lidarseg_labels_filename), \\\n",
    "                'Error: Unable to find {} to load the predictions for sample token {} (lidar ' \\\n",
    "                'sample data token {}) from.'.format(lidarseg_labels_filename, sample_token, pointsensor_token)\n",
    "        else:\n",
    "            if len(semantic_table) > 0:  # Ensure {lidarseg/panoptic}.json is not empty (e.g. in case of v1.0-test).\n",
    "                lidarseg_labels_filename = osp.join(nusc.dataroot,\n",
    "                                                    nusc.get(gt_from, pointsensor_token)['filename'])\n",
    "            else:\n",
    "                lidarseg_labels_filename = None\n",
    "\n",
    "        if lidarseg_labels_filename:\n",
    "            # Paint each label in the pointcloud with a RGBA value.\n",
    "            if show_lidarseg:\n",
    "                coloring = paint_points_label(lidarseg_labels_filename,\n",
    "                                                filter_lidarseg_labels,\n",
    "                                                nusc.lidarseg_name2idx_mapping,\n",
    "                                                nusc.colormap)\n",
    "            else:\n",
    "                coloring = paint_panop_points_label(lidarseg_labels_filename,\n",
    "                                                    filter_lidarseg_labels,\n",
    "                                                    nusc.lidarseg_name2idx_mapping,\n",
    "                                                    nusc.colormap)\n",
    "\n",
    "        else:\n",
    "            coloring = depths\n",
    "            print(f'Warning: There are no lidarseg labels in {nusc.version}. Points will be colored according '\n",
    "                    f'to distance from the ego vehicle instead.')\n",
    "    else:\n",
    "        # Retrieve the color from the depth.\n",
    "        coloring = depths\n",
    "\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths > min_dist)\n",
    "    # mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    # mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    # mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    # mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    height = cam['height']\n",
    "    width = cam['width']\n",
    "    mask = np.logical_and(mask, points[0, :] > 0)\n",
    "    mask = np.logical_and(mask, points[0, :] < width)\n",
    "    mask = np.logical_and(mask, points[1, :] > 0)\n",
    "    mask = np.logical_and(mask, points[1, :] < height)\n",
    "    # Also only take only points inside the box into account\n",
    "    mask = np.logical_and(mask, points_in_box(box, pc.points[:3, :]))\n",
    "    \n",
    "    points = points[:, mask]\n",
    "    coloring = coloring[mask]\n",
    "\n",
    "    return points, coloring #, im\n",
    "\n",
    "\n",
    "def render_pointcloud_to_box(im: np.ndarray, \n",
    "                             nusc: NuScenes,\n",
    "                             sample_token: str,\n",
    "                             pointsensor_token: str,\n",
    "                             camera_token: str,\n",
    "                             box: Box,\n",
    "                             # point_sensor_channel: str ='LIDAR_TOP',\n",
    "                             # camera_channel: str = 'CAM_FRONT',\n",
    "                             min_dist: float = 1.0,\n",
    "                             render_intensity: bool = False,\n",
    "                             show_lidarseg: bool = False,\n",
    "                             filter_lidarseg_labels: List = None,\n",
    "                             lidarseg_preds_bin_path: str = None,\n",
    "                             show_panoptic: bool = False) -> Tuple:\n",
    "    \"\"\"\n",
    "    Given a point sensor (lidar/radar) token and camera sample_data token, load pointcloud and map it to the image\n",
    "    plane.\n",
    "    :param nusc: Active NuScenes object.\n",
    "    :param sample_token: Sample token.\n",
    "    :param pointsensor_token: Lidar/radar sample_data token.\n",
    "    :param camera_token: Camera sample_data token.\n",
    "    :param box: Box that we want map points to.\n",
    "    :param min_dist: Distance from the camera below which points are discarded.\n",
    "    :param render_intensity: Whether to render lidar intensity instead of point depth.\n",
    "    :param show_lidarseg: Whether to render lidar intensity instead of point depth.\n",
    "    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes. If None\n",
    "        or the list is empty, all classes will be displayed.\n",
    "    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation\n",
    "                                    predictions for the sample.\n",
    "    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set\n",
    "        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.\n",
    "        If show_lidarseg is True, show_panoptic will be set to False.\n",
    "    :return (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n",
    "    \"\"\"\n",
    "    points, coloring = map_pointcloud_to_box(\n",
    "        nusc, sample_token=sample_token, pointsensor_token=pointsensor_token, camera_token=camera_token, box=box, min_dist=min_dist,\n",
    "        render_intensity=render_intensity, show_lidarseg=show_lidarseg, filter_lidarseg_labels=filter_lidarseg_labels,\n",
    "        lidarseg_preds_bin_path=lidarseg_preds_bin_path, show_panoptic=show_panoptic)\n",
    "    # Need to be finished later for more precise contours of tracked objects and new objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pixels_from_box_cv2(box: Box,\n",
    "                             width: int,\n",
    "                             height: int,\n",
    "                             view: np.ndarray = np.eye(3),\n",
    "                             normalize: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Renders box using OpenCV2.\n",
    "    :param Box: Box to be rendered\n",
    "    :param width: width of the image\n",
    "    :param height: height of the image\n",
    "    :param view: camera intrinsics\n",
    "    :param normalize: Whether to normalize the remaining coordinate.\n",
    "    \"\"\"\n",
    "\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :]  # all x, y picture coordinates\n",
    "\n",
    "    # simply fill all 6 sides of the cuboid without differentiating which sides are in the fore- or background\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    front = np.ix_([0, 1, 2, 3], [0, 1])\n",
    "    rear = np.ix_([4, 5, 6, 7], [0, 1])\n",
    "    bottom = np.ix_([2, 3, 7, 6], [0, 1])\n",
    "    top = np.ix_([0, 1, 5, 4], [0, 1])\n",
    "    left = np.ix_([0, 3, 7, 4], [0, 1])\n",
    "    right = np.ix_([1, 2, 6, 5], [0, 1])\n",
    "    cv2.fillPoly(mask, pts=[corners.T[front].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[rear].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[bottom].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[top].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[left].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[right].astype(int)], color=(255))\n",
    "\n",
    "    pixels_tuple = np.where(mask == 255)\n",
    "    return mask, pixels_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbvs_bin = \"/home/serov/code/cpp/GBVS_fork/build/gbvs_main\"\n",
    "\n",
    "def get_gbvs_cpp(input_filename):\n",
    "    output_filename = input_filename.replace('image.png', 'gbvs.exr')\n",
    "    gbvs_cmd = gbvs_bin + \" -i \" + input_filename + \" -o \" + output_filename\n",
    "    p = subprocess.Popen(gbvs_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if p.wait() == 0:\n",
    "        return cv2.imread(output_filename, flags=(cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH) )\n",
    "    else:\n",
    "        raise Exception(\"Couldn't create GBVS saliency map.\")\n",
    "\n",
    "def create_gbvs_cpp(input_filename):\n",
    "    output_filename = input_filename.replace('image.png', 'gbvs.exr')\n",
    "    gbvs_cmd = gbvs_bin + \" -i \" + input_filename + \" -o \" + output_filename\n",
    "    p = subprocess.Popen(gbvs_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if p.wait() == 0:\n",
    "        print(\"Created gbvs map at:\", output_filename)\n",
    "    else:\n",
    "        raise Exception(\"Couldn't create GBVS saliency map.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_absolute_error(saliency: np.ndarray,\n",
    "                        groundtruth: np.ndarray) -> float:\n",
    "    assert(saliency.shape == groundtruth.shape)\n",
    "    height, width = saliency.shape\n",
    "    dtype_saliency = saliency.dtype\n",
    "    return 1 / (height * width) * np.sum(np.absolute(saliency - np.asarray(groundtruth / 255.0, dtype=dtype_saliency)))\n",
    "\n",
    "def mean_absolute_error_multi(saliency: np.ndarray,\n",
    "                              mask_new: np.ndarray,\n",
    "                              mask_already: np.ndarray) -> tuple:\n",
    "    assert(saliency.shape == mask_new.shape)\n",
    "    assert(mask_new.shape == mask_already.shape)\n",
    "    height, width = saliency.shape\n",
    "    dtype_saliency = saliency.dtype\n",
    "    indices_new_obj = np.where(mask_new == 255)\n",
    "    num_pixels_new = len(indices_new_obj[0])\n",
    "    saliency_new = saliency[[indices_new_obj[0], indices_new_obj[1]]]\n",
    "\n",
    "    mae_new = 1 / num_pixels_new * np.sum(np.absolute(saliency_new - 1))\n",
    "    mae_already = 1 / (height * width) * np.sum(np.absolute(saliency - np.asarray(mask_already.clip(0, 1).astype(dtype_saliency))))\n",
    "    \n",
    "    return mae_new, mae_already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_image_and_saliency_as_heatmap(im: np.ndarray,\n",
    "                                          saliency: np.ndarray,\n",
    "                                          resize_factor: float) -> np.ndarray:\n",
    "\n",
    "    im_resize = cv2.resize(im, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    saliency_resize = cv2.resize(saliency, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    if im_resize.dtype != np.uint8:\n",
    "        im_resize = np.asarray(im_resize, dtype=np.uint8)\n",
    "    if saliency_resize.dtype != np.uint8:\n",
    "        saliency_resize = np.asarray(saliency_resize * 255, dtype=np.uint8)\n",
    "    if len(im_resize.shape) != len(saliency_resize.shape):\n",
    "        if len(saliency_resize.shape) == 2:  # grayscale\n",
    "            saliency_resize = cv2.cvtColor(saliency_resize, cv2.COLOR_GRAY2BGR)  # convert to color\n",
    "        elif len(saliency_resize).shape == 3:  # color\n",
    "            saliency_resize = cv2.cvtColor(saliency_resize, cv2.COLOR_BGR2GRAY)  # convert to grayscale\n",
    "\n",
    "    saliency_resize = cv2.applyColorMap(saliency_resize, cv2.COLORMAP_JET)\n",
    "\n",
    "    alpha = 0.7\n",
    "    beta = 0.3\n",
    "    gamma = 0\n",
    "    return cv2.addWeighted(im_resize, alpha, saliency_resize, beta, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convex_mask_and_hull_points_from_lidar_points(box_mask: np.ndarray,\n",
    "                           lidar_points_inside_box: np.ndarray,\n",
    "                           width: int,\n",
    "                           height: int) -> Tuple:\n",
    "    # check number of lidar points and overall size of the masked_box vs. the contour created from projected lidar points\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    hull_points = cv2.convexHull(lidar_points_inside_box[:2, :].T.astype(np.float32))\n",
    "  \n",
    "    if hull_points is None:\n",
    "        hull_valid = False\n",
    "    else:\n",
    "        hull_valid = True\n",
    "        # if scale != 1.0:\n",
    "        #     M = cv2.moments(hull_points)\n",
    "        #     cx = int(M['m10']/M['m00'])\n",
    "        #     cy = int(M['m01']/M['m00'])\n",
    "        #     hull_points_norm = hull_points - [cx, cy]\n",
    "        #     hull_points_scaled = hull_points_norm * scale\n",
    "        #     hull_points = hull_points_scaled + [cx, cy]\n",
    "        cv2.fillConvexPoly(mask, hull_points.astype(np.int32), color=(255))\n",
    "\n",
    "    return mask, hull_valid, hull_points\n",
    "\n",
    "def scale_contour(contour: np.ndarray,\n",
    "                  scale: float)-> np.ndarray:\n",
    "    if scale == 1.0:\n",
    "        return contour\n",
    "    else:\n",
    "        M = cv2.moments(contour)\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "        contour_norm = contour - [[cx], [cy]]\n",
    "        contour_scaled = scale * contour_norm \n",
    "        # contour_scaled += [[cy], [cx]]\n",
    "        return contour_scaled, (cy, cy)\n",
    "\n",
    "\n",
    "def center_of_hull_points(hull_points: np.ndarray):\n",
    "    M = cv2.moments(hull_points)\n",
    "    if M['m00'] == 0:\n",
    "        return (None, None)\n",
    "    else:\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "        return (cx, cy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "def box_render_cv2_text(box: Box,\n",
    "                        nusc: NuScenes,\n",
    "                        im: np.ndarray,\n",
    "                        view: np.ndarray = np.eye(3),\n",
    "                        normalize: bool = False,\n",
    "                        colors: Tuple = ((0, 0, 255), (255, 0, 0), (155, 155, 155)),\n",
    "                        linewidth: int = 2,\n",
    "                        render_text: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Renders box using OpenCV2.\n",
    "    :param Box: Box to be rendered\n",
    "    :param nusc: Active NuScenes object\n",
    "    :param im: <np.array: width, height, 3>. Image array. Channels are in BGR order.\n",
    "    :param view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).\n",
    "    :param is_key_frame\n",
    "    :param normalize: Whether to normalize the remaining coordinate.\n",
    "    :param colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.\n",
    "    :param linewidth: Linewidth for plot.\n",
    "    \"\"\"\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :]\n",
    "\n",
    "    def draw_rect(selected_corners, color):\n",
    "        prev = selected_corners[-1]\n",
    "        for corner in selected_corners:\n",
    "            cv2.line(im,\n",
    "                     (int(prev[0]), int(prev[1])),\n",
    "                     (int(corner[0]), int(corner[1])),\n",
    "                     color, linewidth)\n",
    "            prev = corner\n",
    "\n",
    "    # Draw the sides\n",
    "    for i in range(4):\n",
    "        cv2.line(im,\n",
    "                 (int(corners.T[i][0]), int(corners.T[i][1])),\n",
    "                 (int(corners.T[i + 4][0]), int(corners.T[i + 4][1])),\n",
    "                 colors[2][::-1], linewidth)\n",
    "\n",
    "    # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n",
    "    draw_rect(corners.T[:4], colors[0][::-1])\n",
    "    draw_rect(corners.T[4:], colors[1][::-1])\n",
    "\n",
    "    # Draw line indicating the front\n",
    "    center_bottom_forward = np.mean(corners.T[2:4], axis=0)\n",
    "    center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)\n",
    "    cv2.line(im,\n",
    "             (int(center_bottom[0]), int(center_bottom[1])),\n",
    "             (int(center_bottom_forward[0]), int(center_bottom_forward[1])),\n",
    "             colors[0][::-1], linewidth)\n",
    "\n",
    "    # Add some text to the bounding box\n",
    "    # Create text to be written\n",
    "    attribute_tokens =  nusc.get('sample_annotation', box.token)['attribute_tokens']\n",
    "    text = box.name\n",
    "    for attribute_token in attribute_tokens:\n",
    "        attribute = nusc.get('attribute', attribute_token)\n",
    "        text += '\\n' + attribute['name']\n",
    "    visibility = nusc.get('sample_annotation', box.token)['visibility_token']\n",
    "    text += '\\nvisibility ' + visibility\n",
    "    # Setup multi line text\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    font_scale = 0.5\n",
    "    thickness = 1\n",
    "    text_size, _ = cv2.getTextSize(box.name, font, font_scale, thickness)\n",
    "    line_height = text_size[1] + 5\n",
    "    center = np.mean(corners.T[:], axis=0)\n",
    "    y0 = int(center[1])\n",
    "    for i, text_line in enumerate(text.split('\\n')):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(im, text_line, (int(center[0]), y),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, font_scale, colors[0][::-1], thickness, cv2.LINE_AA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningStats():\n",
    "    \"\"\" \n",
    "    see https://github.com/liyanage/python-modules/blob/master/running_stats.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.old_m = 0\n",
    "        self.new_m = 0\n",
    "        self.old_s = 0\n",
    "        self.new_s = 0\n",
    "        self.categories = []\n",
    "        self.mean_absolute_error_new_objects = []\n",
    "        self.mean_absolute_error_already_tracked_objects = []\n",
    "        self.mean_absolute_error_whole_image = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def add_sample(self, x):\n",
    "        self.n += 1\n",
    "\n",
    "        if self.n == 1:\n",
    "            self.old_m = self.new_m = x\n",
    "            self.old_s = 0\n",
    "        else:\n",
    "            self.new_m = self.old_m + (x - self.old_m) / self.n\n",
    "            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n",
    "\n",
    "            self.old_m = self.new_m\n",
    "            self.old_s = self.new_s\n",
    "\n",
    "    def mean(self):\n",
    "        return self.new_m if self.n else 0.0\n",
    "\n",
    "    def variance(self):\n",
    "        return self.new_s / (self.n - 1) if self.n > 1 else 0.0\n",
    "\n",
    "    def standard_deviation(self):\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "class SaliencyEvalStats():\n",
    "    def __init__(self, name) -> None:\n",
    "        # arrays containing relevant data\n",
    "        self.n = int(0)\n",
    "        self.name = name\n",
    "        self.mean_absolute_error_new_objects: float = []\n",
    "        self.mean_absolute_error_already_tracked_objects: float = []\n",
    "    \n",
    "    def add_sample(self, mean_absolute_error_new_object, mean_absolute_error_already_tracked_object):\n",
    "        self.n += 1\n",
    "        self.mean_absolute_error_new_objects.append(mean_absolute_error_new_object)\n",
    "        self.mean_absolute_error_already_tracked_objects.append(mean_absolute_error_already_tracked_object)\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(\"{} n: {:d} mae_new {:0.10f} mae_tracked {:0.10f}\".format(self.name,\n",
    "              self.n, np.mean(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "              np.mean(np.asarray(self.mean_absolute_error_already_tracked_objects))))\n",
    "        print(\"{} n: {:d} std mae_new {:0.10f} std mae_tracked {:0.10f}\".format(self.name,\n",
    "              self.n, np.std(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "              np.std(np.asarray(self.mean_absolute_error_already_tracked_objects))))\n",
    "\n",
    "    def get_stats(self):\n",
    "        return np.array([[np.mean(np.asarray(self.mean_absolute_error_new_objects)),\n",
    "                          np.std(np.asarray(self.mean_absolute_error_new_objects))]\n",
    "                         [np.mean(np.asarray(self.mean_absolute_error_already_tracked_objects)),\n",
    "                          np.std(np.asarray(self.mean_absolute_error_already_tracked_objects))]])\n",
    "\n",
    "    def n(self):\n",
    "        return self.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_im_and_saliency(im: np.ndarray,\n",
    "                            saliency: np.ndarray,\n",
    "                            window_name: str,\n",
    "                            # mask_new: np.ndarray,\n",
    "                            # mask_already: np.ndarray,\n",
    "                            # mae_new: float,\n",
    "                            # mae_already: float\n",
    "                            ) -> None:\n",
    "    (height, width, channels) = im.shape\n",
    "    saliency_heatmap = cv2.cvtColor((255*saliency).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    assert(im.shape == saliency_heatmap.shape)\n",
    "    saliency_heatmap = cv2.applyColorMap(saliency_heatmap, cv2.COLORMAP_JET)\n",
    "    display_im = cv2.addWeighted(im, 0.7, saliency_heatmap, 0.3, 0.0)\n",
    "    cv2.imshow(window_name, display_im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyWindow(window_name)\n",
    "\n",
    "def debug_im_and_saliency(im: np.ndarray,\n",
    "                          saliency: np.ndarray,\n",
    "                          window_name: str,\n",
    "                          mae_new: float,\n",
    "                          mae_already: float\n",
    "                          ) -> None:\n",
    "    (height, width, channels) = im.shape\n",
    "    saliency_heatmap = cv2.cvtColor((255*saliency).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    assert(im.shape == saliency_heatmap.shape)\n",
    "    saliency_heatmap = cv2.applyColorMap(saliency_heatmap, cv2.COLORMAP_JET)\n",
    "    display_im = cv2.addWeighted(im, 0.7, saliency_heatmap, 0.3, 0.0)\n",
    "    cv2_put_multi_line_text(display_im, \"mae_new = {:0.10f}\".format(mae_new), (width/2, height/2), BLACK)\n",
    "    cv2_put_multi_line_text(display_im, \"mae_already = {:0.10f}\".format(mae_already), (width/2, height/2 + 20), BLACK)\n",
    "    cv2.imshow(window_name, display_im)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyWindow(window_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serov/anaconda3/envs/nuscenes_local/lib/python3.7/site-packages/ipykernel_launcher.py:17: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "camera_channel = 'CAM_FRONT'\n",
    "pointsensor_channel = 'LIDAR_TOP'\n",
    "minimum_visibility = 2 # excluding objects that are only visibile 0-40%\n",
    "height = 900\n",
    "width = 1600\n",
    "break_all = False\n",
    "save_already_tracked = True\n",
    "display_debug = False\n",
    "debug = False\n",
    "\n",
    "data_path = \"/data/datasets/nuscenes_results/birth_frames_and_masks\"\n",
    "\n",
    "# create spectral residual object\n",
    "spectral_residual_h8w128 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h8w128.setImageHeight(8)\n",
    "spectral_residual_h8w128.setImageWidth(128)\n",
    "spectral_residual_h64w64 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h64w64.setImageHeight(64)\n",
    "spectral_residual_h64w64.setImageWidth(64)\n",
    "spectral_residual_h128w128 = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "spectral_residual_h128w128.setImageHeight(128)\n",
    "spectral_residual_h128w128.setImageWidth(128)\n",
    "\n",
    "# prepare baseline birth histogram / saliency\n",
    "baseline_histogram_filename = \"/data/datasets/nuscenes_results/object_birth_heatmaps/birth_heatmap_0849.npy\"\n",
    "baseline_birth_histogram = np.load(baseline_histogram_filename)\n",
    "# make baseline horizontally symmetric\n",
    "baseline_birth_histogram = (baseline_birth_histogram + np.fliplr(baseline_birth_histogram)) / 2\n",
    "baseline_birth_histogram = baseline_birth_histogram / np.max(baseline_birth_histogram)\n",
    "base_saliency = baseline_birth_histogram.copy()\n",
    "baseline_birth_histogram_uint8 = (255 * baseline_birth_histogram).astype(np.uint8)\n",
    "baseline_birth_histogram_heatmap = cv2.cvtColor(baseline_birth_histogram_uint8, cv2.COLOR_GRAY2BGR)\n",
    "baseline_birth_histogram_heatmap = cv2.applyColorMap(baseline_birth_histogram_uint8, cv2.COLORMAP_JET)\n",
    "\n",
    "# solo stats\n",
    "base_stats = SaliencyEvalStats(\"base\")\n",
    "gbvs_stats = SaliencyEvalStats(\"gbvs\")\n",
    "spectralh8w128_stats = SaliencyEvalStats(\"spectralh8w128\")\n",
    "spectralh64w64_stats = SaliencyEvalStats(\"spectralh64w64\")\n",
    "spectralh128w128_stats = SaliencyEvalStats(\"spectralh128w128\")\n",
    "# combinations\n",
    "# gbvs + baseline ->  pwp pixel_wise_product\n",
    "# gbvs + baseline ->  ns normalized_and_sum\n",
    "# gbvs + baseline ->  nm normalized_and_maximum\n",
    "# gbvs + baseline ->  cnws coherent_normalization_with_weighted_sum\n",
    "# gbvs + baseline ->  gnla global_nonlinear_amplification\n",
    "# spectralh8w128 + baseline ->  PWP pixel_wise_product\n",
    "# spectralh8w128 + baseline ->  NS normalized_and_sum\n",
    "# spectralh8w128 + baseline ->  NM normalized_and_maximum\n",
    "# spectralh8w128 + baseline ->  CNWS coherent_normalization_with_weighted_sum\n",
    "# spectralh8w128 + baseline ->  GNLA global_nonlinear_amplification\n",
    "gbvs_baseline_pwp_stats = SaliencyEvalStats(\"gbvs_baseline_pwp\")\n",
    "gbvs_baseline_ns_stats = SaliencyEvalStats(\"gbvs_baseline_ns\")\n",
    "gbvs_baseline_nm_stats = SaliencyEvalStats(\"gbvs_baseline_nm\")\n",
    "gbvs_baseline_cnws_stats = SaliencyEvalStats(\"gbvs_baseline_cnws\")\n",
    "gbvs_baseline_gnla_stats = SaliencyEvalStats(\"gbvs_baseline_gnla\")\n",
    "spectralh8w128_baseline_pwp_stats = SaliencyEvalStats(\"spectralh8w128_baseline_pwp\")\n",
    "spectralh8w128_baseline_ns_stats = SaliencyEvalStats(\"spectralh8w128_baseline_ns\")\n",
    "spectralh8w128_baseline_nm_stats = SaliencyEvalStats(\"spectralh8w128_baseline_nm\")\n",
    "spectralh8w128_baseline_cnws_stats = SaliencyEvalStats(\"spectralh8w128_baseline_cnws\")\n",
    "spectralh8w128_baseline_gnla_stats = SaliencyEvalStats(\"spectralh8w128_baseline_gnla\")\n",
    "\n",
    "timestamps: str = []\n",
    "scene_idxs: int = []\n",
    "image_filenames: str = []\n",
    "# categories: int = []\n",
    "\n",
    "for idx_scene, scene in enumerate(nusc.scene):\n",
    "    first_sample_rec = nusc.get('sample', scene['first_sample_token'])\n",
    "    first_sd_rec = nusc.get('sample_data', first_sample_rec['data'][camera_channel])\n",
    "    current_sd_rec = first_sd_rec\n",
    "    print(\"evaluating scene \", idx_scene + 1)\n",
    "    has_more_frames = True\n",
    "    unique_instances = [] # list of all unique objects per scene\n",
    "    while has_more_frames:\n",
    "        # Get annotations, boxes, camera_intrinsic\n",
    "        # When using BoxVisibility.ALL objects are detected too late in certain cases\n",
    "        impath, boxes, camera_intrinsic = nusc.get_sample_data(current_sd_rec['token'], box_vis_level=BoxVisibility.ANY)\n",
    "        if not os.path.exists(impath):\n",
    "            raise Exception('Error: Missing image %s' % impath)\n",
    "\n",
    "        current_im = cv2.imread(impath)\n",
    "        current_im_box = current_im.copy()\n",
    "        timestamp = current_sd_rec['timestamp']\n",
    "\n",
    "        already_tracked_objects_mask = np.zeros((height, width), dtype=int)\n",
    "        new_objects_mask = np.zeros((height, width), dtype=int)\n",
    "        im_contains_unique_instance_and_is_not_first_frame = False\n",
    "        # new_objects_categories_and_masks = []\n",
    "        if current_sd_rec['is_key_frame']:\n",
    "            for box in boxes:\n",
    "                sample_annotation = nusc.get('sample_annotation', box.token)\n",
    "                instance = nusc.get('instance', sample_annotation['instance_token'])\n",
    "                category = nusc.get('category', instance['category_token'])\n",
    "                # Exclude certain static objects, e.g. debris or traffic cones\n",
    "                if category['index'] > 8 and category['index'] < 14:\n",
    "                    continue\n",
    "                # Only keep objects with certain visibility\n",
    "                if int(sample_annotation['visibility_token']) < minimum_visibility:\n",
    "                    continue\n",
    "                skip_box_due_to_attribute = False\n",
    "                for attribute_token in sample_annotation['attribute_tokens']:\n",
    "                    attribute_name = nusc.get('attribute', attribute_token)['name']\n",
    "                    if 'without_rider' in attribute_name or 'sitting' in attribute_name:\n",
    "                        skip_box_due_to_attribute = True\n",
    "                if skip_box_due_to_attribute:\n",
    "                    continue\n",
    "                \n",
    "                # Gather unique instances\n",
    "                instance_token = sample_annotation['instance_token']\n",
    "                if instance_token not in unique_instances:\n",
    "                    im_contains_unique_instance = True\n",
    "                    unique_instances.append(instance_token)\n",
    "                    if timestamp != first_sd_rec['timestamp']:\n",
    "                        # This box can mean a potential object birth\n",
    "                        im_contains_unique_instance_and_is_not_first_frame = True\n",
    "                        # full bounding box\n",
    "                        box_mask, _ = mask_pixels_from_box_cv2(box, width, height, view=camera_intrinsic, normalize=True)\n",
    "                        \n",
    "                        new_objects_mask = new_objects_mask + box_mask\n",
    "                        box_render_cv2_text(box, nusc, current_im_box, view=camera_intrinsic, normalize=True, colors=(GREEN, GREEN, GREEN), render_text=False)\n",
    "                        # this will be done in another evaluation\n",
    "                        # new_objects_categories_and_masks.append(tuple([category, box_mask]))\n",
    "                else:\n",
    "                    box_mask, _ = mask_pixels_from_box_cv2(box, width, height, view=camera_intrinsic, normalize=True)\n",
    "                    already_tracked_objects_mask = already_tracked_objects_mask + box_mask\n",
    "                    # c = nusc.explorer.get_color(box.name)\n",
    "                    box_render_cv2_text(box, nusc, current_im_box, view=camera_intrinsic, normalize=True,\n",
    "                                        colors=(RED, RED, RED), render_text=False)\n",
    "\n",
    "            # only evaluate frames that are keyframes, are not first frames and contain new unique instances \n",
    "            if im_contains_unique_instance_and_is_not_first_frame:\n",
    "                timestamps.append(timestamp)\n",
    "                scene_idxs.append(idx_scene + 1)\n",
    "                image_filenames.append(impath)\n",
    "                # create mask usable in cv2\n",
    "                new_objects_mask = new_objects_mask.clip(0, 255).astype(np.uint8)\n",
    "                already_tracked_objects_mask = already_tracked_objects_mask.clip(0, 255).astype(np.uint8)\n",
    "                if save_already_tracked:\n",
    "                    already_tracked_filename = data_path + os.sep + \"s{:04d}_{}_already_tracked.png\".format(idx_scene + 1, timestamp)\n",
    "                    cv2.imwrite(already_tracked_filename, already_tracked_objects_mask)\n",
    "                # prepare different saliency base maps\n",
    "                gbvs_filename = data_path + os.sep + \"s{:04d}_{}_gbvs.exr\".format(idx_scene + 1, timestamp)\n",
    "                gbvs_saliency = cv2.imread(gbvs_filename, flags=(cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH))\n",
    "                if gbvs_saliency is None:\n",
    "                    print(\"Couldn't read gbvs saliency for \", gbvs_filename)\n",
    "                (spectralh8w128_success, spectralh8w128_saliency) = spectral_residual_h8w128.computeSaliency(current_im)\n",
    "                if not spectralh8w128_success:\n",
    "                    print(\"Couldn't create saliency spectralh8w128 for \", impath)\n",
    "                    continue\n",
    "                (spectralh64w64_success, spectralh64w64_saliency) = spectral_residual_h64w64.computeSaliency(current_im)\n",
    "                if not spectralh64w64_success:\n",
    "                    print(\"Couldn't create saliency spectralh64w64 for \", impath)\n",
    "                    continue\n",
    "                (spectralh128w128_success, spectralh128w128_saliency) = spectral_residual_h128w128.computeSaliency(current_im)\n",
    "                if not spectralh128w128_success:\n",
    "                    print(\"Couldn't create saliency spectralh128w128 for \", impath)\n",
    "                    continue\n",
    "                # create all gbvs combinations\n",
    "                gbvs_base_pwp_saliency = simple_pixel_wise_product(gbvs_saliency, base_saliency)\n",
    "                gbvs_base_ns_saliency = normalized_and_sum(gbvs_saliency, base_saliency)\n",
    "                gbvs_base_nm_saliency = normalized_and_maximum(gbvs_saliency, base_saliency)\n",
    "                gbvs_base_cnws_saliency = coherent_normalization_with_weighted_sum(gbvs_saliency, base_saliency)\n",
    "                # create all spectral residual combinations\n",
    "                spectralh8w128_base_pwp_saliency = simple_pixel_wise_product(spectralh8w128_saliency, base_saliency)\n",
    "                spectralh8w128_base_ns_saliency = normalized_and_sum(spectralh8w128_saliency, base_saliency)\n",
    "                spectralh8w128_base_nm_saliency = normalized_and_maximum(spectralh8w128_saliency, base_saliency)\n",
    "                spectralh8w128_base_cnws_saliency = coherent_normalization_with_weighted_sum(spectralh8w128_saliency, base_saliency)\n",
    "                # spectralh8w128_base_gnla_saliency = global_nonlinear_amplification(spectralh8w128_saliency, base_saliency)\n",
    "                # Calculate solo all errors\n",
    "                mae_new_base, mae_already_base = mean_absolute_error_multi(base_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_gbvs, mae_already_gbvs = mean_absolute_error_multi(gbvs_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_spectralh8w128, mae_already_spectralh8w128 = mean_absolute_error_multi(spectralh8w128_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_spectralh64w64, mae_already_spectralh64w64 = mean_absolute_error_multi(spectralh64w64_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_spectralh128w128, mae_already_spectralh128w128 = mean_absolute_error_multi(spectralh128w128_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                # calculate all gbvs combination errors\n",
    "                mae_new_gbvs_base_pwp, mae_already_gbvs_base_pwp = mean_absolute_error_multi(gbvs_base_pwp_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_gbvs_base_ns, mae_already_gbvs_base_ns = mean_absolute_error_multi(gbvs_base_ns_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_gbvs_base_nm, mae_already_gbvs_base_nm = mean_absolute_error_multi(gbvs_base_nm_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_gbvs_base_cnws, mae_already_gbvs_base_cnws = mean_absolute_error_multi(gbvs_base_cnws_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                # mae_new_gbvs_base_gnla, mae_already_gbvs_base_gnla = mean_absolute_error_multi(gbvs_base_gnla_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                # calculate all spectralh8w128 combination errors\n",
    "                mae_new_gbvs_base_pwp, mae_already_gbvs_base_pwp = mean_absolute_error_multi(gbvs_base_pwp_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_gbvs_base_ns, mae_already_gbvs_base_ns = mean_absolute_error_multi(gbvs_base_ns_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_gbvs_base_nm, mae_already_gbvs_base_nm = mean_absolute_error_multi(gbvs_base_nm_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                mae_new_gbvs_base_cnws, mae_already_gbvs_base_cnws = mean_absolute_error_multi(gbvs_base_cnws_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                # mae_new_gbvs_base_gnla, mae_already_gbvs_base_gnla = mean_absolute_error_multi(gbvs_base_gnla_saliency, new_objects_mask, already_tracked_objects_mask)\n",
    "                \n",
    "                # add all solo errors\n",
    "                base_stats.add_sample(mae_new_base, mae_already_base)\n",
    "                gbvs_stats.add_sample(mae_new_gbvs, mae_already_gbvs)\n",
    "                spectralh8w128_stats.add_sample(mae_new_spectralh8w128, mae_already_spectralh8w128)\n",
    "                spectralh64w64_stats.add_sample(mae_new_spectralh64w64, mae_already_spectralh64w64)\n",
    "                spectralh128w128_stats.add_sample(mae_new_spectralh128w128, mae_already_spectralh128w128)\n",
    "                # add gbvs combination errors\n",
    "                gbvs_baseline_pwp_stats.add_sample(mae_new_gbvs_base_pwp, mae_already_gbvs_base_pwp)\n",
    "                gbvs_baseline_ns_stats.add_sample(mae_new_gbvs_base_ns, mae_already_gbvs_base_ns)\n",
    "                gbvs_baseline_nm_stats.add_sample(mae_new_gbvs_base_nm, mae_already_gbvs_base_nm)\n",
    "                gbvs_baseline_cnws_stats.add_sample(mae_new_gbvs_base_cnws, mae_already_gbvs_base_cnws)\n",
    "                # gbvs_baseline_gnla_stats.add_sample(mae_new_gbvs_base_gnla, mae_already_gbvs_base_gnla)\n",
    "                # add spectral combination errors\n",
    "                spectralh8w128_baseline_pwp_stats.add_sample(mae_new_gbvs_base_pwp, mae_already_gbvs_base_pwp)\n",
    "                spectralh8w128_baseline_ns_stats.add_sample(mae_new_gbvs_base_ns, mae_already_gbvs_base_ns)\n",
    "                spectralh8w128_baseline_nm_stats.add_sample(mae_new_gbvs_base_nm, mae_already_gbvs_base_nm)\n",
    "                spectralh8w128_baseline_cnws_stats.add_sample(mae_new_gbvs_base_cnws, mae_already_gbvs_base_cnws)\n",
    "                # spectralh8w128_baseline_gnla_stats.add_sample(mae_new_gbvs_base_gnla, mae_already_gbvs_base_gnla)\n",
    "                \n",
    "                # indices_already_tracked_objects = np.where(already_tracked_objects_mask == 255)\n",
    "                # already_tracked_objects_mask = cv2.cvtColor(already_tracked_objects_mask, cv2.COLOR_GRAY2BGR)\n",
    "                # if len(indices_already_tracked_objects[0]) > 0:\n",
    "                #     already_tracked_objects_mask[indices_already_tracked_objects[0], indices_already_tracked_objects[1]] = (0, 0, 255)\n",
    "                \n",
    "                # display solos\n",
    "                if debug:\n",
    "                    debug_im_and_saliency(current_im_box, base_saliency, \"base_saliency\", mae_new_base, mae_already_base)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_saliency, \"gbvs_saliency\", mae_new_gbvs, mae_already_gbvs)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_saliency, \"spectral_residual_h8w128\", mae_new_spectralh8w128, mae_already_spectralh8w128)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh64w64_saliency, \"spectral_residual_h64w64\", mae_new_spectralh64w64, mae_already_spectralh64w64)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh128w128_saliency, \"spectral_residual_h128w128\", mae_new_spectralh128w128, mae_already_spectralh128w128)\n",
    "                    # display gbvs combinations\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_pwp_saliency, \"gbvs_base_pwp_saliency\", mae_new_gbvs_base_pwp, mae_already_gbvs_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_ns_saliency, \"gbvs_base_ns_saliency\", mae_new_gbvs_base_ns, mae_already_gbvs_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_nm_saliency, \"gbvs_base_nm_saliency\", mae_new_gbvs_base_nm, mae_already_gbvs_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, gbvs_base_cnws_saliency, \"gbvs_base_cnws_saliency\", mae_new_gbvs_base_cnws, mae_already_gbvs_base_cnws)\n",
    "                    # display spectral combinations\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_pwp_saliency, \"spectralh8w128_base_pwp_saliency\", mae_new_gbvs_base_pwp, mae_already_gbvs_base_pwp)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_ns_saliency, \"spectralh8w128_base_ns_saliency\", mae_new_gbvs_base_ns, mae_already_gbvs_base_ns)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_nm_saliency, \"spectralh8w128_base_nm_saliency\", mae_new_gbvs_base_nm, mae_already_gbvs_base_nm)\n",
    "                    debug_im_and_saliency(current_im_box, spectralh8w128_base_cnws_saliency, \"spectralh8w128_base_cnws_saliency\", mae_new_gbvs_base_cnws, mae_already_gbvs_base_cnws)\n",
    "\n",
    "\n",
    "                if display_debug:\n",
    "                    new_objects_mask = cv2.cvtColor(new_objects_mask, cv2.COLOR_GRAY2BGR)\n",
    "                    already_tracked_objects_mask = cv2.cvtColor(already_tracked_objects_mask, cv2.COLOR_GRAY2BGR)\n",
    "                    display_im = cv2.addWeighted(current_im, 0.7, new_objects_mask, 0.3, 0.0)\n",
    "                    display_im = cv2.addWeighted(display_im, 0.7, already_tracked_objects_mask, 0.3, 0.0)\n",
    "                    # debug all images\n",
    "\n",
    "                    cv2.imshow(\"im, new and tracked objects\", display_im)\n",
    "                    cv2.imshow(\"already tracked objects\", already_tracked_objects_mask)\n",
    "                    if im_contains_unique_instance_and_is_not_first_frame:\n",
    "                        key = cv2.waitKey(1)\n",
    "                    else:\n",
    "                        key = cv2.waitKey(1)\n",
    "\n",
    "                    if key == 27:\n",
    "                        break_all = True\n",
    "                \n",
    "        if break_all:\n",
    "            break\n",
    "        \n",
    "        if current_sd_rec['next'] == '':\n",
    "            has_more_frames = False\n",
    "        else:\n",
    "            current_sd_rec = nusc.get('sample_data', current_sd_rec['next'])\n",
    "        \n",
    "    if break_all:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base n: 6 mae_new 0.5384140354 mae_tracked 0.1574167004\n",
      "base n: 6 std mae_new 0.1272476241 std mae_tracked 0.0484817936\n",
      "gbvs n: 6 mae_new 0.6460381433 mae_tracked 0.2524076624\n",
      "gbvs n: 6 std mae_new 0.2334152145 std mae_tracked 0.0587122937\n",
      "spectralh8w128 n: 6 mae_new 0.8599498257 mae_tracked 0.2284926975\n",
      "spectralh8w128 n: 6 std mae_new 0.0684930334 std mae_tracked 0.0654311086\n",
      "spectralh64w64 n: 6 mae_new 0.7939806879 mae_tracked 0.2125915220\n",
      "spectralh64w64 n: 6 std mae_new 0.1054069832 std mae_tracked 0.0617612148\n",
      "spectralh128w128 n: 6 mae_new 0.8555461269 mae_tracked 0.1586147569\n",
      "spectralh128w128 n: 6 std mae_new 0.0752102921 std mae_tracked 0.0650312459\n",
      "gbvs_baseline_pwp n: 6 mae_new 0.6763595714 mae_tracked 0.1208717472\n",
      "gbvs_baseline_pwp n: 6 std mae_new 0.1391551519 std mae_tracked 0.0606002217\n",
      "gbvs_baseline_ns n: 6 mae_new 0.3730019979 mae_tracked 0.2514374730\n",
      "gbvs_baseline_ns n: 6 std mae_new 0.1064569331 std mae_tracked 0.0428828592\n",
      "gbvs_baseline_nm n: 6 mae_new 0.4198077580 mae_tracked 0.2813238934\n",
      "gbvs_baseline_nm n: 6 std mae_new 0.0637937975 std mae_tracked 0.0481216206\n",
      "gbvs_baseline_cnws n: 6 mae_new 0.4606426455 mae_tracked 0.2116821065\n",
      "gbvs_baseline_cnws n: 6 std mae_new 0.1177702045 std mae_tracked 0.0463324987\n",
      "spectralh8w128_baseline_pwp n: 6 mae_new 0.6763595714 mae_tracked 0.1208717472\n",
      "spectralh8w128_baseline_pwp n: 6 std mae_new 0.1391551519 std mae_tracked 0.0606002217\n",
      "spectralh8w128_baseline_ns n: 6 mae_new 0.3730019979 mae_tracked 0.2514374730\n",
      "spectralh8w128_baseline_ns n: 6 std mae_new 0.1064569331 std mae_tracked 0.0428828592\n",
      "spectralh8w128_baseline_nm n: 6 mae_new 0.4198077580 mae_tracked 0.2813238934\n",
      "spectralh8w128_baseline_nm n: 6 std mae_new 0.0637937975 std mae_tracked 0.0481216206\n",
      "spectralh8w128_baseline_cnws n: 6 mae_new 0.4606426455 mae_tracked 0.2116821065\n",
      "spectralh8w128_baseline_cnws n: 6 std mae_new 0.1177702045 std mae_tracked 0.0463324987\n",
      "base n: 6 mae_new 0.5384140354 mae_tracked 0.1574167004\n",
      "base n: 6 std mae_new 0.1272476241 std mae_tracked 0.0484817936\n"
     ]
    }
   ],
   "source": [
    "# save pickle objects\n",
    "import pickle\n",
    "base_pickle_path = \"/data/datasets/nuscenes_results/eval_keyframes\"\n",
    "with open(base_pickle_path + os.sep + \"timestamps.pkl\", 'wb') as outp:\n",
    "    pickle.dump(timestamps, outp, pickle.HIGHEST_PROTOCOL)    \n",
    "with open(base_pickle_path + os.sep + \"scene_idxs.pkl\", 'wb') as outp:\n",
    "    pickle.dump(scene_idxs, outp, pickle.HIGHEST_PROTOCOL)    \n",
    "with open(base_pickle_path + os.sep + \"image_filenames.pkl\", 'wb') as outp:\n",
    "    pickle.dump(image_filenames, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"base_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(base_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"gbvs_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(gbvs_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"spectralh8w128_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(spectralh8w128_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"spectralh64w64_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(spectralh64w64_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"spectralh128w128_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(spectralh128w128_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"gbvs_baseline_pwp_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(gbvs_baseline_pwp_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"gbvs_baseline_ns_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(gbvs_baseline_ns_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"gbvs_baseline_nm_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(gbvs_baseline_nm_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"gbvs_baseline_cnws_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(gbvs_baseline_cnws_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"spectralh8w128_baseline_pwp_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(spectralh8w128_baseline_pwp_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"spectralh8w128_baseline_ns_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(spectralh8w128_baseline_ns_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"spectralh8w128_baseline_nm_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(spectralh8w128_baseline_nm_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "with open(base_pickle_path + os.sep + \"spectralh8w128_baseline_cnws_stats.pkl\", 'wb') as outp:\n",
    "    pickle.dump(spectralh8w128_baseline_cnws_stats, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# print stats\n",
    "base_stats.print_stats()\n",
    "gbvs_stats.print_stats()\n",
    "spectralh8w128_stats.print_stats()\n",
    "spectralh64w64_stats.print_stats()\n",
    "spectralh128w128_stats.print_stats()\n",
    "gbvs_baseline_pwp_stats.print_stats()\n",
    "gbvs_baseline_ns_stats.print_stats()\n",
    "gbvs_baseline_nm_stats.print_stats()\n",
    "gbvs_baseline_cnws_stats.print_stats()\n",
    "spectralh8w128_baseline_pwp_stats.print_stats()\n",
    "spectralh8w128_baseline_ns_stats.print_stats()\n",
    "spectralh8w128_baseline_nm_stats.print_stats()\n",
    "spectralh8w128_baseline_cnws_stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# # TODO save MAE stats for every sample to find outliers and investigate distribution of data\n",
    "\n",
    "# width = 1600\n",
    "# height = 900\n",
    "# display_image = True\n",
    "# save_images = True\n",
    "# # get all images and birth masks\n",
    "# base_data_path = \"/data/datasets/nuscenes_results/birth_frames_and_masks\"\n",
    "\n",
    "# image_filenames = sorted(glob.glob(base_data_path + os.sep + \"*image*\", recursive=False))\n",
    "# mask_filenames = sorted(glob.glob(base_data_path + os.sep + \"*mask*\", recursive=False))\n",
    "# gbvs_filenames = sorted(glob.glob(base_data_path + os.sep + \"*gbvs*\", recursive=False))\n",
    "\n",
    "# assert(len(image_filenames) == len(mask_filenames) and len(gbvs_filenames) == len(mask_filenames))\n",
    "\n",
    "# # create spectral residual object\n",
    "# spectral_residual_height = 8\n",
    "# spectral_residual_width = 256\n",
    "# spectral_residual = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "# spectral_residual.setImageHeight(spectral_residual_height)\n",
    "# spectral_residual.setImageWidth(spectral_residual_width)\n",
    "\n",
    "# # prepare object birth histogram\n",
    "# baseline_histogram_filename = \"/data/datasets/nuscenes_results/object_birth_heatmaps/birth_heatmap_0849.npy\"\n",
    "# baseline_birth_histogram = np.load(baseline_histogram_filename)\n",
    "# baseline_birth_histogram = normalize(baseline_birth_histogram)\n",
    "\n",
    "# # saliency maps to be evaluated\n",
    "# saliency_types = ['baseline', 'gbvs', 'spectral_residual_h8w256']\n",
    "# # Combinations?\n",
    "# saliency_results = {}\n",
    "# for saliency_type in saliency_types:\n",
    "#     saliency_results[saliency_type] = RunningStats()\n",
    "\n",
    "# for image_filename, mask_filename, gbvs_filename in zip(image_filenames, mask_filenames, gbvs_filenames):\n",
    "#     img = cv2.imread(image_filename)\n",
    "#     mask = cv2.imread(mask_filename, flags=cv2.IMREAD_GRAYSCALE)\n",
    "#     gbvs = cv2.imread(gbvs_filename, flags=(cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH))\n",
    "#     (success, saliency_spectral_residual) = spectral_residual.computeSaliency(img)\n",
    "#     if not success:\n",
    "#         print(\"Couldn't create saliency for \", image_filename)\n",
    "#         continue\n",
    "    \n",
    "#     img_and_gbvs = overlay_image_and_saliency_as_heatmap(img, gbvs, 1.0)\n",
    "#     img_and_baseline = overlay_image_and_saliency_as_heatmap(img, baseline_birth_histogram, 1.0)\n",
    "#     # saliency_spectral_residual = cv2.cvtColor((saliency_spectral_residual*255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "#     img_and_spectral_residual = overlay_image_and_saliency_as_heatmap(img, normalize(saliency_spectral_residual), 1.0)\n",
    "\n",
    "#     mask_bgr = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "#     img_and_gbvs_and_mask = cv2.addWeighted(img_and_gbvs, 0.7, mask_bgr, 0.3, 0.0)\n",
    "#     img_and_baseline_and_mask = cv2.addWeighted(img_and_baseline, 0.7, mask_bgr, 0.3, 0.0)\n",
    "#     img_and_spectral_residual_and_mask = cv2.addWeighted(img_and_spectral_residual, 0.7, mask_bgr, 0.3, 0.0)\n",
    "\n",
    "#     mae_baseline = mean_absolute_error(baseline_birth_histogram, mask)\n",
    "#     mae_gbvs = mean_absolute_error(gbvs, mask)\n",
    "#     mae_spectral_residual = mean_absolute_error(saliency_spectral_residual, mask)#\n",
    "\n",
    "#     saliency_results['baseline'].add_sample(mae_baseline)\n",
    "#     saliency_results['gbvs'].add_sample(mae_gbvs)\n",
    "#     saliency_results['spectral_residual_h8w256'].add_sample(mae_spectral_residual)\n",
    "\n",
    "#     mae_baseline = mean_absolute_error(baseline_birth_histogram, mask)\n",
    "#     cv2_put_multi_line_text(img_and_gbvs_and_mask, str(mae_gbvs), (width/2, height/2), (0, 0, 0))\n",
    "#     cv2_put_multi_line_text(img_and_baseline_and_mask, str(mae_baseline), (width/2, height/2), (0, 0, 0))\n",
    "#     cv2.imshow(\"input img and gbvs overlayed\", img_and_spectral_residual_and_mask)\n",
    "#     key = cv2.waitKey(0)\n",
    "#     print(\"MEAN base {} gbvs {} spectral {}\".format(\n",
    "#         saliency_results['baseline'].mean(),\n",
    "#         saliency_results['gbvs'].mean(),\n",
    "#         saliency_results['spectral_residual_h8w256'].mean()))\n",
    "#     print(\"STD  base {} gbvs {} spectral {}\\n\".format(\n",
    "#         saliency_results['baseline'].standard_deviation(),\n",
    "#         saliency_results['gbvs'].standard_deviation(),\n",
    "#         saliency_results['spectral_residual_h8w256'].standard_deviation()))\n",
    "#     if key == 27:  # if ESC is pressed, exit.\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_im' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11481/2544630764.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mconvex_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvex_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_GRAY2BGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mconvex_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices_contour\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_contour\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfinal_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvex_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcv2_put_multi_line_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvex_hull_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlidar_points_inside_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_im' is not defined"
     ]
    }
   ],
   "source": [
    "# convex mask code\n",
    "sample_record = nusc.get('sample', current_sd_rec['sample_token'])\n",
    "sample_token = current_sd_rec['sample_token']\n",
    "pointsensor_token = sample_record['data'][pointsensor_channel]\n",
    "camera_token = sample_record['data'][camera_channel]\n",
    "lidar_points_inside_box, coloring = map_pointcloud_to_box(nusc, sample_token, pointsensor_token, camera_token, box)\n",
    "lidar_points_inside_box = lidar_points_inside_box[:2, :]\n",
    "convex_mask, convex_hull_valid, hull_points = convex_mask_and_hull_points_from_lidar_points(\n",
    "    box_mask, lidar_points_inside_box, width, height)\n",
    "indices_contour = np.where(convex_mask == 255)\n",
    "convex_mask = cv2.cvtColor(convex_mask, cv2.COLOR_GRAY2BGR)\n",
    "convex_mask[indices_contour[0], indices_contour[1]] = (0, 0, 255)\n",
    "final_im = cv2.addWeighted(final_im, 0.7, convex_mask, 0.3, 0.0)\n",
    "cv2_put_multi_line_text(final_im, str(convex_hull_valid), (width/2, height/2), color=(255, 0, 0))\n",
    "for p in lidar_points_inside_box.T:\n",
    "    cv2.drawMarker(final_im, (p[0].astype(int), p[1].astype(int)), color=(\n",
    "        0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(a + [[1], [2]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "318541fadf8f6473a5ac117b97e36e34b5dead6f3ba376595b86165903d1f35a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('nuscenes_local': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
