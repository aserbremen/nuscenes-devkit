{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-panoptic...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "34149 panoptic,\n",
      "Done loading in 45.436 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.8 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os.path as osp\n",
    "import cv2\n",
    "# from nuscenes import NuScenes\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import BoxVisibility\n",
    "\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot='/data/datasets/nuscenes', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from nuscenes.utils.geometry_utils import view_points\n",
    "from nuscenes.utils.data_classes import Box\n",
    "\n",
    "def cv2_put_multi_line_text(im: np.ndarray,\n",
    "                            text: str,\n",
    "                            center: Tuple,\n",
    "                            color: Tuple,\n",
    "                            font=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                            font_scale=0.5,\n",
    "                            thickness=1) -> None:\n",
    "    text_size, _ = cv2.getTextSize(\"dummy_text\", font, font_scale, thickness)\n",
    "    line_height = text_size[1] + 5\n",
    "    y0 = int(center[1])\n",
    "    for i, text_line in enumerate(text.split('\\n')):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(im, text_line, (int(center[0]), y),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, font_scale, color[::-1], thickness, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud\n",
    "from nuscenes.lidarseg.lidarseg_utils import paint_points_label\n",
    "from nuscenes.panoptic.panoptic_utils import paint_panop_points_label\n",
    "from pyquaternion import Quaternion\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def map_pointcloud_to_image_box(nusc: NuScenes,\n",
    "                        pointsensor_token: str,\n",
    "                        camera_token: str,\n",
    "                        min_dist: float = 1.0,\n",
    "                        render_intensity: bool = False,\n",
    "                        show_lidarseg: bool = False,\n",
    "                        filter_lidarseg_labels: List = None,\n",
    "                        lidarseg_preds_bin_path: str = None,\n",
    "                        show_panoptic: bool = False) -> Tuple:\n",
    "    \"\"\"\n",
    "    Given a point sensor (lidar/radar) token and camera sample_data token, load pointcloud and map it to the image\n",
    "    plane.\n",
    "    :param pointsensor_token: Lidar/radar sample_data token.\n",
    "    :param camera_token: Camera sample_data token.\n",
    "    :param min_dist: Distance from the camera below which points are discarded.\n",
    "    :param render_intensity: Whether to render lidar intensity instead of point depth.\n",
    "    :param show_lidarseg: Whether to render lidar intensity instead of point depth.\n",
    "    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes. If None\n",
    "        or the list is empty, all classes will be displayed.\n",
    "    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation\n",
    "                                    predictions for the sample.\n",
    "    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set\n",
    "        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.\n",
    "        If show_lidarseg is True, show_panoptic will be set to False.\n",
    "    :return (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n",
    "    \"\"\"\n",
    "\n",
    "    cam = nusc.nusc.get('sample_data', camera_token)\n",
    "    pointsensor = nusc.nusc.get('sample_data', pointsensor_token)\n",
    "    pcl_path = osp.join(nusc.nusc.dataroot, pointsensor['filename'])\n",
    "    if pointsensor['sensor_modality'] == 'lidar':\n",
    "        if show_lidarseg or show_panoptic:\n",
    "            gt_from = 'lidarseg' if show_lidarseg else 'panoptic'\n",
    "            assert hasattr(nusc.nusc, gt_from), f'Error: nuScenes-{gt_from} not installed!'\n",
    "\n",
    "            # Ensure that lidar pointcloud is from a keyframe.\n",
    "            assert pointsensor['is_key_frame'], \\\n",
    "                'Error: Only pointclouds which are keyframes have lidar segmentation labels. Rendering aborted.'\n",
    "\n",
    "            assert not render_intensity, 'Error: Invalid options selected. You can only select either ' \\\n",
    "                                            'render_intensity or show_lidarseg, not both.'\n",
    "\n",
    "        pc = LidarPointCloud.from_file(pcl_path)\n",
    "    else:\n",
    "        pc = RadarPointCloud.from_file(pcl_path)\n",
    "    im = Image.open(osp.join(nusc.nusc.dataroot, cam['filename']))\n",
    "\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = pc.points[2, :]\n",
    "\n",
    "    if render_intensity:\n",
    "        assert pointsensor['sensor_modality'] == 'lidar', 'Error: Can only render intensity for lidar, ' \\\n",
    "                                                            'not %s!' % pointsensor['sensor_modality']\n",
    "        # Retrieve the color from the intensities.\n",
    "        # Performs arbitary scaling to achieve more visually pleasing results.\n",
    "        intensities = pc.points[3, :]\n",
    "        intensities = (intensities - np.min(intensities)) / (np.max(intensities) - np.min(intensities))\n",
    "        intensities = intensities ** 0.1\n",
    "        intensities = np.maximum(0, intensities - 0.5)\n",
    "        coloring = intensities\n",
    "    elif show_lidarseg or show_panoptic:\n",
    "        assert pointsensor['sensor_modality'] == 'lidar', 'Error: Can only render lidarseg labels for lidar, ' \\\n",
    "                                                            'not %s!' % pointsensor['sensor_modality']\n",
    "\n",
    "        gt_from = 'lidarseg' if show_lidarseg else 'panoptic'\n",
    "        semantic_table = getattr(nusc.nusc, gt_from)\n",
    "\n",
    "        if lidarseg_preds_bin_path:\n",
    "            sample_token = nusc.nusc.get('sample_data', pointsensor_token)['sample_token']\n",
    "            lidarseg_labels_filename = lidarseg_preds_bin_path\n",
    "            assert os.path.exists(lidarseg_labels_filename), \\\n",
    "                'Error: Unable to find {} to load the predictions for sample token {} (lidar ' \\\n",
    "                'sample data token {}) from.'.format(lidarseg_labels_filename, sample_token, pointsensor_token)\n",
    "        else:\n",
    "            if len(semantic_table) > 0:  # Ensure {lidarseg/panoptic}.json is not empty (e.g. in case of v1.0-test).\n",
    "                lidarseg_labels_filename = osp.join(nusc.nusc.dataroot,\n",
    "                                                    nusc.nusc.get(gt_from, pointsensor_token)['filename'])\n",
    "            else:\n",
    "                lidarseg_labels_filename = None\n",
    "\n",
    "        if lidarseg_labels_filename:\n",
    "            # Paint each label in the pointcloud with a RGBA value.\n",
    "            if show_lidarseg:\n",
    "                coloring = paint_points_label(lidarseg_labels_filename,\n",
    "                                                filter_lidarseg_labels,\n",
    "                                                nusc.nusc.lidarseg_name2idx_mapping,\n",
    "                                                nusc.nusc.colormap)\n",
    "            else:\n",
    "                coloring = paint_panop_points_label(lidarseg_labels_filename,\n",
    "                                                    filter_lidarseg_labels,\n",
    "                                                    nusc.nusc.lidarseg_name2idx_mapping,\n",
    "                                                    nusc.nusc.colormap)\n",
    "\n",
    "        else:\n",
    "            coloring = depths\n",
    "            print(f'Warning: There are no lidarseg labels in {nusc.nusc.version}. Points will be colored according '\n",
    "                    f'to distance from the ego vehicle instead.')\n",
    "    else:\n",
    "        # Retrieve the color from the depth.\n",
    "        coloring = depths\n",
    "\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths > min_dist)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    points = points[:, mask]\n",
    "    coloring = coloring[mask]\n",
    "\n",
    "    return points, coloring, im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "font_scale = 0.5\n",
    "thickness = 1\n",
    "\n",
    "def box_render_cv2_text(box: Box,\n",
    "                    nusc: NuScenes,\n",
    "                    im: np.ndarray,\n",
    "                    view: np.ndarray = np.eye(3),\n",
    "                    normalize: bool = False,\n",
    "                    colors: Tuple = ((0, 0, 255), (255, 0, 0), (155, 155, 155)),\n",
    "                    linewidth: int = 2,\n",
    "                    ) -> None:\n",
    "    \"\"\"\n",
    "    Renders box using OpenCV2.\n",
    "    :param Box: Box to be rendered\n",
    "    :param nusc: Active NuScenes object\n",
    "    :param im: <np.array: width, height, 3>. Image array. Channels are in BGR order.\n",
    "    :param view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).\n",
    "    :param is_key_frame\n",
    "    :param normalize: Whether to normalize the remaining coordinate.\n",
    "    :param colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.\n",
    "    :param linewidth: Linewidth for plot.\n",
    "    \"\"\"\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :]\n",
    "\n",
    "    def draw_rect(selected_corners, color):\n",
    "        prev = selected_corners[-1]\n",
    "        for corner in selected_corners:\n",
    "            cv2.line(im,\n",
    "                     (int(prev[0]), int(prev[1])),\n",
    "                     (int(corner[0]), int(corner[1])),\n",
    "                     color, linewidth)\n",
    "            prev = corner\n",
    "\n",
    "    # Draw the sides\n",
    "    for i in range(4):\n",
    "        cv2.line(im,\n",
    "                 (int(corners.T[i][0]), int(corners.T[i][1])),\n",
    "                 (int(corners.T[i + 4][0]), int(corners.T[i + 4][1])),\n",
    "                 colors[2][::-1], linewidth)\n",
    "\n",
    "    # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)\n",
    "    draw_rect(corners.T[:4], colors[0][::-1])\n",
    "    draw_rect(corners.T[4:], colors[1][::-1])\n",
    "\n",
    "    # Draw line indicating the front\n",
    "    center_bottom_forward = np.mean(corners.T[2:4], axis=0)\n",
    "    center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)\n",
    "    cv2.line(im,\n",
    "             (int(center_bottom[0]), int(center_bottom[1])),\n",
    "             (int(center_bottom_forward[0]), int(center_bottom_forward[1])),\n",
    "             colors[0][::-1], linewidth)\n",
    "\n",
    "    # Add some text to the bounding box\n",
    "    # Create text to be written\n",
    "    attribute_tokens =  nusc.get('sample_annotation', box.token)['attribute_tokens']\n",
    "    text = box.name\n",
    "    for attribute_token in attribute_tokens:\n",
    "        attribute = nusc.get('attribute', attribute_token)\n",
    "        text += '\\n' + attribute['name']\n",
    "    visibility = nusc.get('sample_annotation', box.token)['visibility_token']\n",
    "    text += '\\nvisibility ' + visibility\n",
    "    # Setup multi line text\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    font_scale = 0.5\n",
    "    thickness = 1\n",
    "    text_size, _ = cv2.getTextSize(box.name, font, font_scale, thickness)\n",
    "    line_height = text_size[1] + 5\n",
    "    center = np.mean(corners.T[:], axis=0)\n",
    "    y0 = int(center[1])\n",
    "    for i, text_line in enumerate(text.split('\\n')):\n",
    "        y = y0 + i * line_height\n",
    "        cv2.putText(im, text_line, (int(center[0]), y),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, font_scale, colors[0][::-1], thickness, cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "def mask_pixels_from_box_cv2(box: Box,\n",
    "                      width: int,\n",
    "                      height: int,\n",
    "                      view: np.ndarray = np.eye(3),\n",
    "                      normalize: bool = False\n",
    "                      ) -> None:\n",
    "    \"\"\"\n",
    "    Renders box using OpenCV2.\n",
    "    :param Box: Box to be rendered\n",
    "    :param nusc: Active NuScenes object\n",
    "    :param im: <np.array: width, height, 3>. Image array. Channels are in BGR order.\n",
    "    :param view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).\n",
    "    :param is_key_frame\n",
    "    :param normalize: Whether to normalize the remaining coordinate.\n",
    "    :param colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.\n",
    "    :param linewidth: Linewidth for plot.\n",
    "    \"\"\"\n",
    "\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :] # all x, y picture coordinates\n",
    "\n",
    "    # simply fill all 6 sides of the cuboid without differentiating which sides are in the fore- or background\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    front = np.ix_([0, 1, 2, 3], [0, 1])\n",
    "    rear = np.ix_([4, 5, 6, 7], [0, 1]) \n",
    "    bottom = np.ix_([2, 3, 7, 6], [0, 1])\n",
    "    top = np.ix_([0, 1, 5, 4], [0, 1])\n",
    "    left = np.ix_([0, 3, 7, 4], [0, 1])\n",
    "    right = np.ix_([1, 2, 6, 5], [0, 1])\n",
    "    cv2.fillPoly(mask, pts=[corners.T[front].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[rear].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[bottom].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[top].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[left].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[right].astype(int)], color=(255))\n",
    "    \n",
    "    pixels_tuple = np.where(mask == 255)\n",
    "    # pixels = np.array(())\n",
    "    # pixels = np.transpose(np.asarray(mask == 255).nonzero())\n",
    "    return mask, pixels_tuple\n",
    "\n",
    "\n",
    "\n",
    "def contour_from_box_cv2(box: Box,\n",
    "                      width: int,\n",
    "                      height, int,\n",
    "                      view: np.ndarray = np.eye(3),\n",
    "                      normalize: bool = False\n",
    "                      ) -> None:\n",
    "    \"\"\"\n",
    "    Renders box using OpenCV2.\n",
    "    :param Box: Box to be rendered\n",
    "    :param nusc: Active NuScenes object\n",
    "    :param im: <np.array: width, height, 3>. Image array. Channels are in BGR order.\n",
    "    :param view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).\n",
    "    :param is_key_frame\n",
    "    :param normalize: Whether to normalize the remaining coordinate.\n",
    "    :param colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.\n",
    "    :param linewidth: Linewidth for plot.\n",
    "    \"\"\"\n",
    "\n",
    "    corners = view_points(box.corners(), view, normalize=normalize)[:2, :]  # all x, y picture coordinates\n",
    "\n",
    "    # simply fill all 6 sides of the cuboid without differentiating which sides are in the fore- or background\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    front = np.ix_([0, 1, 2, 3], [0, 1])\n",
    "    rear = np.ix_([4, 5, 6, 7], [0, 1])\n",
    "    bottom = np.ix_([2, 3, 7, 6], [0, 1])\n",
    "    top = np.ix_([0, 1, 5, 4], [0, 1])\n",
    "    left = np.ix_([0, 3, 7, 4], [0, 1])\n",
    "    right = np.ix_([1, 2, 6, 5], [0, 1])\n",
    "    cv2.fillPoly(mask, pts=[corners.T[front].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[rear].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[bottom].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[top].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[left].astype(int)], color=(255))\n",
    "    cv2.fillPoly(mask, pts=[corners.T[right].astype(int)], color=(255))\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_birth_and_prebirth(prev_im: np.ndarray,\n",
    "                              current_im: np.ndarray,\n",
    "                              resize_factor: float) -> np.ndarray:\n",
    "    \n",
    "    prev_im_shrink = cv2.resize(prev_im, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    current_im_shrink = cv2.resize(current_im, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    return np.vstack([prev_im_shrink, current_im_shrink])\n",
    "\n",
    "def render_birth_and_heatmap(current_im: np.ndarray,\n",
    "                             heatmap: np.ndarray,\n",
    "                             resize_factor: float) -> np.ndarray:\n",
    "\n",
    "    current_im_shrink = cv2.resize(current_im, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    heatmap_shrink = cv2.resize(heatmap, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    heatmap_shrink = np.uint8(heatmap_shrink * 255 / np.max(heatmap_shrink))\n",
    "    heatmap_shrink = cv2.applyColorMap(heatmap_shrink, cv2.COLORMAP_JET)\n",
    "    return np.vstack([current_im_shrink, heatmap_shrink])\n",
    "\n",
    "def overlay_birth_and_heatmap(current_im: np.ndarray, \n",
    "                              heatmap: np.ndarray,\n",
    "                              resize_factor: float) ->np.ndarray:\n",
    "\n",
    "    current_im_shrink = cv2.resize(current_im, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    heatmap_shrink = cv2.resize(heatmap, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_AREA)\n",
    "    alpha = 0.8\n",
    "    beta = 0.5\n",
    "    gamma = 1.0\n",
    "    return cv2.addWeighted(current_im_shrink, alpha, heatmap_shrink, beta, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nuscenes.utils.geometry_utils import points_in_box\n",
    "\n",
    "channel = 'CAM_FRONT'\n",
    "\n",
    "width = 1600\n",
    "height = 900\n",
    "heatmap_histogramm = np.zeros((height, width), dtype=np.float64)\n",
    "\n",
    "for idx_scene, scene in enumerate(nusc.scene):\n",
    "    first_sample_rec = nusc.get('sample', scene['first_sample_token'])\n",
    "    first_sd_rec = nusc.get('sample_data', first_sample_rec['data'][channel])\n",
    "    current_sd_rec = first_sd_rec\n",
    "    prev_sd_rec = None\n",
    "    prev_im = None\n",
    "\n",
    "    window_name= '{}'.format(scene['name'])\n",
    "    # cv2.namedWindow(window_name)\n",
    "    # cv2.moveWindow(window_name, 0, 0)\n",
    "    \n",
    "    has_more_frames = True\n",
    "    break_all = False\n",
    "    break_scene = False\n",
    "    milliseconds = 0\n",
    "    unique_instances = []\n",
    "    minimum_visibility = 2\n",
    "    while has_more_frames:\n",
    "\n",
    "        # Get annotations, boxes, camera_intrinsic\n",
    "        # When using BoxVisibility.ALL objects are detected too late in certain cases\n",
    "        impath, boxes, camera_intrinsic = nusc.get_sample_data(current_sd_rec['token'], box_vis_level=BoxVisibility.ANY)\n",
    "        if not osp.exists(impath):\n",
    "            raise Exception('Error: Missing image %s' % impath)\n",
    "\n",
    "        current_im = cv2.imread(impath)\n",
    "        if current_sd_rec['is_key_frame']:\n",
    "            cv2_put_multi_line_text(current_im, \"KEYFRAME\", (int(current_im.shape[1]/2), int(current_im.shape[0]/2-80)), (255, 0, 0))\n",
    "        im_contains_unique_instance = False\n",
    "        is_first_frame = False\n",
    "        # if prev_im is None:\n",
    "        #     prev_im = current_im.copy()\n",
    "        #     is_first_frame = True\n",
    "        #     cv2_put_multi_line_text(prev_im, \"First frame\", (int(prev_im.shape[1]/2), int(prev_im.shape[0]/2-25)), (0, 0, 0))\n",
    "        #     cv2_put_multi_line_text(current_im, \"First frame\", (int(prev_im.shape[1]/2), int(prev_im.shape[0]/2-25)), (0, 0, 0))\n",
    "\n",
    "        for box in boxes:\n",
    "            sample_annotation = nusc.get('sample_annotation', box.token)\n",
    "            instance = nusc.get('instance', sample_annotation['instance_token'])\n",
    "            category = nusc.get('category', instance['category_token'])\n",
    "            # Exclude certain objects\n",
    "            if category['index'] > 8 and category['index'] < 14:\n",
    "                continue\n",
    "            # Only keep objects with certain visibility\n",
    "            if int(sample_annotation['visibility_token']) < minimum_visibility:\n",
    "                continue\n",
    "            skip_box = False\n",
    "            for attribute_token in sample_annotation['attribute_tokens']:\n",
    "                attribute_name = nusc.get('attribute', attribute_token)['name']\n",
    "                if 'without_rider' in attribute_name or 'sitting' in attribute_name:\n",
    "                    skip_box = True\n",
    "            if skip_box:\n",
    "                continue\n",
    "\n",
    "            # Gather unique instances\n",
    "            instance_token = sample_annotation['instance_token']\n",
    "\n",
    "            # print(current_sd_rec)\n",
    "            # if instance_token not in unique_instances:\n",
    "            #     im_contains_unique_instance = True\n",
    "            #     unique_instances.append(instance_token)\n",
    "            #     if current_sd_rec['timestamp'] != first_sd_rec['timestamp']:\n",
    "                    # This box can mean a potential object birth\n",
    "            c = nusc.explorer.get_color(box.name)\n",
    "            box_render_cv2_text(box, nusc, current_im, view=camera_intrinsic, normalize=True, colors=(c, c, c))\n",
    "            mask, pixels_tuple = mask_pixels_from_box_cv2(box, width, height, view=camera_intrinsic, normalize=True)\n",
    "            current_im[pixels_tuple[0], pixels_tuple[1]] = (0, 0, 0)\n",
    "            heatmap_histogramm[pixels_tuple[0], pixels_tuple[1]] += 1\n",
    "            \n",
    "        # birth_im = render_birth_and_prebirth(current_im, heatmap_histogramm, 0.7)\n",
    "        current_im_and_heatmap = render_birth_and_heatmap(current_im, heatmap_histogramm, 0.7)\n",
    "        cv2.imshow(\"birth image and heatmap\", current_im_and_heatmap)\n",
    "        if im_contains_unique_instance or is_first_frame:\n",
    "            key = cv2.waitKey(1)\n",
    "        else:\n",
    "            key = cv2.waitKey(1)\n",
    "        if key == 27:  # if ESC is pressed, exit.\n",
    "            cv2.destroyAllWindows()\n",
    "            break_all = True\n",
    "            break_scene = True\n",
    "            break\n",
    "\n",
    "        # cv2.imshow(window_name, current_im)\n",
    "        # key = cv2.waitKey(milliseconds)\n",
    "        # if key == 32: # if space is pressed, pause.\n",
    "        #     key = cv2.waitKey()\n",
    "        # if key == 27: # if ESC is pressed, exit.\n",
    "        #     break_all = True\n",
    "        #     cv2.destroyAllWindows()\n",
    "        #     break\n",
    "        # if key == 110: # if n is pressed, skip current scene.\n",
    "        #     cv2.destroyAllWindows()\n",
    "        #     break\n",
    "\n",
    "        if break_scene:\n",
    "            break\n",
    "\n",
    "        if current_sd_rec['next'] == '':\n",
    "            has_more_frames = False\n",
    "            # cv2.destroyAllWindows()\n",
    "        else:\n",
    "            prev_im = current_im    \n",
    "            prev_sd_rec = current_sd_rec\n",
    "            current_sd_rec = nusc.get('sample_data', current_sd_rec['next'])\n",
    "\n",
    "    filename_im = \"/home/serov/code/python/nuscenes-devkit/results/object_heatmaps/birth_heatmap_\" + '{0:04d}'.format(idx_scene) + \".png\"\n",
    "    cv2.imwrite(filename_im, cv2.applyColorMap(np.uint8(heatmap_histogramm * 255 / np.max(heatmap_histogramm)), cv2.COLORMAP_JET))\n",
    "    filename_npy = \"/home/serov/code/python/nuscenes-devkit/results/object_heatmaps/birth_heatmap_\" + \\\n",
    "        '{0:04d}'.format(idx_scene) + \".npy\"\n",
    "    np.save(filename_npy, heatmap_histogramm)\n",
    "    if break_all:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "318541fadf8f6473a5ac117b97e36e34b5dead6f3ba376595b86165903d1f35a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('nuscenes_local': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
